{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PLS_sandbox.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TomMaullin/BLMM-sandbox/blob/master/PLS_sandbox.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIMtrhFKB3Ay",
        "colab_type": "text"
      },
      "source": [
        "# PLS implementation in python\n",
        "\n",
        "This code implements the PLS algorithm for estimating the parameters of linear mixed effects models as described in [Bates (2015)](https://www.jstatsoft.org/article/view/v067i01/v67i01.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNFQ0-MpQuBm",
        "colab_type": "text"
      },
      "source": [
        "## Pip Installations\n",
        "\n",
        "Pip install everything.**bold text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UX02P0KvBWJr",
        "colab_type": "code",
        "outputId": "283df0f8-5ce1-4784-e064-c25f12513e6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        }
      },
      "source": [
        "!pip install numpy\n",
        "!pip install cvxopt\n",
        "!pip install pandas\n",
        "!pip install scipy\n",
        "!pip install matplotlib\n",
        "!pip install sparse\n",
        "!pip install nilearn\n",
        "!pip install nibabel"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.16.4)\n",
            "Requirement already satisfied: cvxopt in /usr/local/lib/python3.6/dist-packages (1.2.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.24.2)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.5.3)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.16.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.16.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.0.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.16.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.5.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib) (41.0.1)\n",
            "Requirement already satisfied: sparse in /usr/local/lib/python3.6/dist-packages (0.7.0)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.6/dist-packages (from sparse) (1.3.0)\n",
            "Requirement already satisfied: numba>=0.39 in /usr/local/lib/python3.6/dist-packages (from sparse) (0.40.1)\n",
            "Requirement already satisfied: numpy>=1.13 in /usr/local/lib/python3.6/dist-packages (from sparse) (1.16.4)\n",
            "Requirement already satisfied: llvmlite>=0.25.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.39->sparse) (0.29.0)\n",
            "Requirement already satisfied: nilearn in /usr/local/lib/python3.6/dist-packages (0.5.2)\n",
            "Requirement already satisfied: nibabel>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from nilearn) (2.3.3)\n",
            "Requirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.6/dist-packages (from nibabel>=2.0.2->nilearn) (1.16.4)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from nibabel>=2.0.2->nilearn) (1.12.0)\n",
            "Requirement already satisfied: bz2file in /usr/local/lib/python3.6/dist-packages (from nibabel>=2.0.2->nilearn) (0.98)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.6/dist-packages (2.3.3)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from nibabel) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.6/dist-packages (from nibabel) (1.16.4)\n",
            "Requirement already satisfied: bz2file in /usr/local/lib/python3.6/dist-packages (from nibabel) (0.98)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4JYRICVBtjl",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Python Imports\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkTBWbRKQ5ah",
        "colab_type": "text"
      },
      "source": [
        "We need:\n",
        " - `numpy` for matrix handling.\n",
        " - `scipy` for minimising the likelihood.\n",
        " - `cvxopt` for sparse cholesky.\n",
        " - `pandas` for quick reading and writing of csv files.\n",
        " - `os` for basic commandline functions\n",
        " - `time` for timing functions.\n",
        " - `matplotlib` for making displays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tebSlxvBBruv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "8e3c8780-a3ce-4e88-ac66-c991d26cd78c"
      },
      "source": [
        "import numpy as np\n",
        "import cvxopt\n",
        "from cvxopt import cholmod, umfpack, amd, matrix, spmatrix, lapack\n",
        "import pandas as pd\n",
        "from scipy.optimize import root\n",
        "from scipy.optimize import minimize\n",
        "import scipy.sparse\n",
        "import os\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "import sparse\n",
        "import nibabel as nib\n",
        "import nilearn"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilAB3qmDMHa8",
        "colab_type": "text"
      },
      "source": [
        "## Helper Functions\n",
        "\n",
        "This section contains miscellaneous functions used to help the `PLS` function as well as the general manipulation of the Sparse Cholesky decomposition given by `cvxopt`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOtDwtrfQk_Y",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Sparse Cholesky Decomposition function\n",
        "\n",
        "This function takes in a square matrix **M** and outputs **P** and **L** from it's sparse cholesky decomposition of the form **PAP'=LL**'.\n",
        "\n",
        "Note: P is given as a permutation vector rather than a matrix.\n",
        " \n",
        "---\n",
        " \n",
        "The following inputs are required for this function:\n",
        "\n",
        "---\n",
        "\n",
        " - **M**: The matrix to be sparse cholesky decomposed as an spmatrix from the cvxopt package.\n",
        " - **perm**: Input permutation (*optional*, one will be calculated if not)\n",
        " - **retF**: Return the factorisation object or not\n",
        " - **retP**: Return the permutation or not\n",
        " - **retL**: Return the lower cholesky or not\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpyEbgrhNHzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sparse_chol(M, perm=None, retF=False, retP=True, retL=True):\n",
        "\n",
        "    # Quick check that M is square\n",
        "    if M.size[0]!=M.size[1]:\n",
        "        raise Exception('M must be square.')\n",
        "\n",
        "    # Set the factorisation to use LL' instead of LDL'\n",
        "    cholmod.options['supernodal']=2\n",
        "\n",
        "    if not perm is None:\n",
        "        # Make an expression for the factorisation\n",
        "        F=cholmod.symbolic(M,p=perm)\n",
        "    else:\n",
        "        # Make an expression for the factorisation\n",
        "        F=cholmod.symbolic(M)\n",
        "\n",
        "    # Calculate the factorisation\n",
        "    cholmod.numeric(M, F)\n",
        "\n",
        "    # Empty factorisation object\n",
        "    factorisation = {}\n",
        "\n",
        "    if retF:\n",
        "\n",
        "        # Calculate the factorisation again (buggy if returning L for\n",
        "        # some reason)\n",
        "        F2=cholmod.symbolic(M,p=perm)\n",
        "        cholmod.numeric(M, F2)\n",
        "\n",
        "        # If we want to return the F object, add it to the dictionary\n",
        "        factorisation['F']=F2\n",
        "\n",
        "    if retP:\n",
        "\n",
        "        # Set p to [0,...,n-1]\n",
        "        P = cvxopt.matrix(range(M.size[0]), (M.size[0],1), tc='d')\n",
        "\n",
        "        # Solve and replace p with the true permutation used\n",
        "        cholmod.solve(F, P, sys=7)\n",
        "\n",
        "        # Convert p into an integer array; more useful that way\n",
        "        P=cvxopt.matrix(np.array(P).astype(np.int64),tc='i')\n",
        "\n",
        "        # If we want to return the permutation, add it to the dictionary\n",
        "        factorisation['P']=P\n",
        "\n",
        "    if retL:\n",
        "\n",
        "        # Get the sparse cholesky factor\n",
        "        L=cholmod.getfactor(F)\n",
        "        \n",
        "        # If we want to return the factor, add it to the dictionary\n",
        "        factorisation['L']=L\n",
        "\n",
        "    # Return P and L\n",
        "    return(factorisation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xYlGmnNNnit",
        "colab_type": "text"
      },
      "source": [
        "### Inverse mapping function\n",
        "\n",
        "This function takes in a lower triangular \n",
        "block diagonal matrix, lambda, and maps it to the original vector of parameters, theta.\n",
        "\n",
        "---\n",
        "The following inputs are required for this function:\n",
        "\n",
        "---\n",
        "\n",
        " - **Lambda**: The sparse lower triangular block diagonal matrix.\n",
        " \n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59GHirQ8OGe7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inv_mapping(Lambda):\n",
        "\n",
        "    # List the unique elements of lambda (in the\n",
        "    # correct order; pandas does this, numpy does\n",
        "    # not)\n",
        "    theta = pd.unique(list(cvxopt.spmatrix.trans(Lambda)))\n",
        "    return(theta)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsYGm--5isJY",
        "colab_type": "text"
      },
      "source": [
        "### Calculate mapping function\n",
        "\n",
        "This function takes in a vector of parameters, theta, and returns indices which maps them the to lower triangular block diagonal matrix, lambda.\n",
        "\n",
        "---\n",
        "The following inputs are required for this function:\n",
        "\n",
        "---\n",
        "\n",
        " - **nlevels**: a vector of the number of levels for each grouping factor. e.g. nlevels=[10,2] means there are 10 levels for factor 1 and 2 levels for factor 2.\n",
        " - **nparams**: a vector of the number of variables for each grouping factor. e.g. nparams=[3,4] means there are 3 variables for factor 1 and 4 variables for factor 2.\n",
        "\n",
        "All arrays must be np arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgZfxqOUitYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_mapping(nlevels, nparams):\n",
        "\n",
        "    # Work out how many factors there are\n",
        "    n_f = len(nlevels)\n",
        "\n",
        "    # Quick check that nlevels and nparams are the same length\n",
        "    if len(nlevels)!=len(nparams):\n",
        "        raise Exception('The number of parameters and number of levels should be recorded for every grouping factor.')\n",
        "\n",
        "    # Work out how many lambda components needed for each factor\n",
        "    n_lamcomps = (np.multiply(nparams,(nparams+1))/2).astype(np.int64)\n",
        "\n",
        "    # Block index is the index of the next un-indexed diagonal element\n",
        "    # of Lambda\n",
        "    block_index = 0\n",
        "\n",
        "    # Row indices and column indices of theta\n",
        "    row_indices = np.array([])\n",
        "    col_indices = np.array([])\n",
        "\n",
        "    # This will have the values of theta repeated several times, once\n",
        "    # for each time each value of theta appears in lambda\n",
        "    theta_repeated_inds = np.array([])\n",
        "    \n",
        "    # Loop through factors generating the indices to map theta to.\n",
        "    for i in range(0,n_f):\n",
        "\n",
        "        # Work out the indices of a lower triangular matrix\n",
        "        # of size #variables(factor) by #variables(factor)\n",
        "        row_inds_tri, col_inds_tri = np.tril_indices(nparams[i])\n",
        "\n",
        "        # Work out theta for this block\n",
        "        theta_current_inds = np.arange(np.sum(n_lamcomps[0:i]),np.sum(n_lamcomps[0:(i+1)]))\n",
        "\n",
        "        # Work out the repeated theta\n",
        "        theta_repeated_inds = np.hstack((theta_repeated_inds, np.tile(theta_current_inds, nlevels[i])))\n",
        "\n",
        "        # For each level of the factor we must repeat the lower\n",
        "        # triangular matrix\n",
        "        for j in range(0,nlevels[i]):\n",
        "\n",
        "            # Append the row/column indices to the running list\n",
        "            row_indices = np.hstack((row_indices, (row_inds_tri+block_index)))\n",
        "            col_indices = np.hstack((col_indices, (col_inds_tri+block_index)))\n",
        "\n",
        "            # Move onto the next block\n",
        "            block_index = block_index + nparams[i]\n",
        "\n",
        "    # Create lambda as a sparse matrix\n",
        "    #lambda_theta = spmatrix(theta_repeated.tolist(), row_indices.astype(np.int64), col_indices.astype(np.int64))\n",
        "\n",
        "    # Return lambda\n",
        "    return(theta_repeated_inds, row_indices, col_indices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdRkv7PrrgsI",
        "colab_type": "text"
      },
      "source": [
        "### Apply mapping function\n",
        "\n",
        "The below function applies a mapping to a vector of parameters.\n",
        "\n",
        "---\n",
        "The following inputs are required for this function:\n",
        "\n",
        "---\n",
        "\n",
        " - **theta**: the vector of theta parameters.\n",
        " - **theta_inds**: A vector specifying how many times each theta parameter should be repeated. For example, if theta=[0.1,0.8,0.3] and theta_inds=[1,1,1,2,3,3], then the values to be mapped into the sparse matrix would be [0.1,0.1,0.1,0.8,0.3,0.3].\n",
        " - **r_inds**: The row indices of the elements mapped into the sparse matrix.\n",
        " - **c_inds**: The column indices of the elements mapped into the sparse matrix.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxUv8_r0rhFv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mapping(theta, theta_inds, r_inds, c_inds):\n",
        "\n",
        "    return(spmatrix(theta[theta_inds.astype(np.int64)].tolist(), r_inds.astype(np.int64), c_inds.astype(np.int64)))\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS5zhoWCDZ8E",
        "colab_type": "text"
      },
      "source": [
        "## Toy Dataset\n",
        "\n",
        "This section read ins and formats a toy dataset. The files used here were generated in `R` and with **True** values (those with postfix `True`) being those used to generate the data and **Estimated** (those with postfix `REst`) values being the estimates `R`'s `lmer` package generated from this data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qy39zwuhkn4C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make a data directory\n",
        "if not os.path.isdir('/Data'):\n",
        "  os.mkdir('/Data')\n",
        "  \n",
        "os.chdir('/Data')\n",
        "\n",
        "# Clone small git repo containg some csv files.\n",
        "if not os.path.isdir('/Data/BLMM-testdata'):\n",
        "  !git clone https://github.com/TomMaullin/BLMM-testdata.git\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EWsCZjCQcc9",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Z matrix\n",
        "\n",
        "The below reads in Z and makes an image of Z transpose.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoKOwHqcEKDT",
        "colab_type": "code",
        "outputId": "a9308826-d430-4d61-b0ad-5d249666d91e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "# Read in random effects design matrix and convert it into it's sparse format in\n",
        "# cvxopt.\n",
        "Z_3col=pd.read_csv('/Data/BLMM-testdata/Z_3col.csv',header=None).values\n",
        "Z = cvxopt.spmatrix(Z_3col[:,2].tolist(), (Z_3col[:,0]-1).astype(np.int64), \\\n",
        "                    (Z_3col[:,1]-1).astype(np.int64))\n",
        "\n",
        "# Create an image of Z'\n",
        "imshow(np.array(cvxopt.matrix(cvxopt.spmatrix.trans(Z))), \\\n",
        "       interpolation='nearest', vmin=-5, vmax=5, aspect='auto')\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fa0eca65ba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAFnNJREFUeJzt3X2QXFWZx/Hfk5kkM3khLxrHJENM\nhoQgIhswQCIuOyVi2Ihi1VIqo27czW5E1zWIL4C6peyqBborYolAMO5EpAwIrLCRZWAx6KIYSEyD\ngQA9BJAkkPAOCQKZ5Nk/+nbP7Z5+f5mZzPl+qqa677n3nnPu7dPP3LndzxxzdwEAwjBqqDsAABg8\nBH0ACAhBHwACQtAHgIAQ9AEgIAR9AAgIQR8AAkLQB4CA1BT0zexUM3vIzHrN7Lx6dQoA0BhWbUau\nmTVJeljSKZK2S7pH0pnu/kChfZomjvfmaZOrai+jjz9OAITl9Se2P+Pu0+pRV3MN+x4vqdfdt0mS\nma2VdLqkgkG/edpktX/r0zU0KR3Y1VLT/gBwsHns7C88Xq+6arlsninpidjy9qgMADBMNfxeiZmt\nMLONZrbxwMt7G90cAKCIWoL+DkmHxpbbo7Is7r7K3Re6+8JRE8fX0BwAoFa13NO/R9I8M5ujVLD/\niKSuYju8feKzGv+T0Rpzyz3q2ZnQfa+/qqueW6z7jnX17ExoyYwFkqSenQlJ0pIZCzLP08u9Fy+q\nocsAELaqg76795nZZyT1SGqS9GN3v79uPQMA1F0tV/py95sl3VynvgAAGowvvQNAQKpOzqrG2FmH\n+ozPn11THaPaXq1pf77nD+Bg89jZX9jk7gvrURdX+gAQEII+AASEoA8AASHoA0BAavrK5lBou2Gs\nxl+3QZIyCV09OxN63wmnqe+J7ZnytCUzFmj3p9+pzV/9IcldAILHlT4ABISgDwABIegDQEAI+gAQ\nkIMuI7dWtWb0SmT1AhhcZOQCAKpC0AeAgBD0ASAgBH0ACEhwQb910zh1dCU0/q7U44eP3KSOroSS\nnd3q6ErouLc8rv1Pt2TKJt3emlmXLgOAg1VwQR8AQkbQB4CAEPQBICDBJWfVAwleAAYTyVkAgKoQ\n9AEgIAR9AAgIQR8AAnLQTZc4HMz73E7dvPlWLZmxQFJqesbP7DhBP5i5ITN9Y/pRkj65fbF+88tj\nNOuC32XKDrvmrCHrP4BwcaUPAAEh6ANAQAj6ABAQgj4ABISM3CFCVi+AcpGRCwCoCkEfAAJC0AeA\ngBD0ASAgZOQOkY6uxICydLaupEy2b/vvJ2j1rDslSae+5Xj5vtclSa+edry2nzwIHQUwonClDwAB\nIegDQEBKBn0z+7GZ7TazLbGyqWZ2m5klo8cpje0mAKAeSiZnmdlJkvZI+om7HxWVfVvSc+5+oZmd\nJ2mKu59bqjGSs+qr1gQvkruAg8OgJme5+28kPZdTfLqkNdHzNZI+WI/OAAAaq9p7+m3u/mT0/ClJ\nbXXqDwCggWr+INdT94cK3iMysxVmttHMNu7fs7fW5gAANag26O8ys+mSFD3uLrShu69y94XuvrBp\nwvgqmwMA1EO1yVk3SVom6cLo8ca69Qhlm/nT0brjR1dqn+/X0b/7hLaeeFXWdI3SwISv+LreixcN\nSb8BDJ1yvrL5M0l3SZpvZtvNbLlSwf4UM0tKek+0DAAY5kpe6bv7mQVW8U8AAOAgQ0YuAASEoA8A\nAWG6xIAxZSNwcGC6RABAVQj6ABAQgj4ABISgDwABYbrEgCU7uzPPl8xYIN3erp63rutfVn9G75IZ\nC7Ti4W1adXhHpuydnztLu0jqBQ4qXOkDQEAI+gAQEII+AASE5CzUhAQvoPFIzgIAVIWgDwABIegD\nQEAI+gAQEJKzUJOOrlSiVs/OhE7Z+n6NOvmJrCka04740ae0+mOX6l87jpUkXfmnOzWreYKWzFjA\ntI3AIOJKHwACQtAHgIAQ9AEgIAR9AAgIGbkYcmT1AsWRkQsAqApBHwACQtAHgIAQ9AEgIGTkYsgl\nO7v1rWfm69dHt6pnZ0JLZizIPKb1XnWM5n58s6RU9u/Sh5bq5vk3Z7YhqxcoD1f6ABAQgj4ABISg\nDwABITkLI0KtCV4kd2E4IzkLAFAVgj4ABISgDwABIegDQEBIzsKIcGh3s16b1KTfXnJFWdunE8DS\nz0nuQii40geAgBD0ASAgBH0ACEjJoG9mh5rZejN7wMzuN7OVUflUM7vNzJLR45TGdxcAUIuSGblm\nNl3SdHf/g5lNlLRJ0gclfULSc+5+oZmdJ2mKu59brC4ycjFcMWUjhrNBzch19yfd/Q/R85clbZU0\nU9LpktZEm61R6hcBAGAYq+ievpnNlnSMpA2S2tz9yWjVU5LaCuyzwsw2mtnG/Xv21tBVAECtyg76\nZjZB0vWSznb3l+LrPHWPKO99Indf5e4L3X1h04TxNXUWAFCbsoK+mY1WKuBf7e43RMW7ovv96fv+\nuxvTRQBAvZTMyDUzk7Ra0lZ3/25s1U2Slkm6MHq8sSE9BAZBR1cqOzedpXvqrIV69OojNfvD92Vl\n7v759OPVeuPdWdM6ph12zVmD33GgQuX8G4YTJX1c0h/NLD3Cv6xUsL/WzJZLelzShxrTRQBAvZQM\n+u5+pyQrsPrk+nYHANBIZOQCQECYLhGoExK80ChMlwgAqApBHwACQtAHgIAQ9AEgIEyXCNRJsrNb\nUv9UjPFHqT/x68m+PfrErHdl9iPBC4OJK30ACAhBHwACQtAHgIAQ9AEgIGTkAsMIWb3Ih4xcAEBV\nCPoAEBCCPgAEhKAPDCPJzm4lO7vV0ZXI/CQ7u9X3WlOmfN43XpHvaM2sO7DfNG/lDiU7uzX5ttah\nPgQMcwR9AAgIQR8AAkLQB4CAEPQBICAkZwEjTK0JXiR3DT8kZwEAqkLQB4CAEPQBICAEfQAICNMl\nAiPMzxdfoXPnnKDP9j6o7889Qj07E7p2zyR9aMKLkpSZvjFt1FFH6H9uXZsp77140aD3GYOHK30A\nCAhBHwACQtAHgIAQ9AEgIGTkAsjClI3DDxm5AICqEPQBICAEfQAICMlZALIkO7uzltNJWz07E3nL\n4+tI8Br+uNIHgIAQ9AEgIAR9AAhIyaBvZi1mdreZ3Wtm95vZBVH5HDPbYGa9ZnaNmY1pfHcBALUo\nmZxlZiZpvLvvMbPRku6UtFLSOZJucPe1Zna5pHvd/bJidZGcBYSBBK/6GtTkLE/ZEy2Ojn5c0rsl\nXReVr5H0wXp0CADQOGXd0zezJjNLSNot6TZJj0h6wd37ok22S5rZmC4CAOqlrKDv7vvdfYGkdknH\nSzqi3AbMbIWZbTSzjfv37K2ymwCAeqjo2zvu/oKk9ZIWS5psZunkrnZJOwrss8rdF7r7wqYJ42vq\nLACgNiUzcs1smqR97v6CmbVKOkXSRUoF/zMkrZW0TNKNjewogIPH5ENe0Qltjyt53GuSpK9sS+ik\nllTGbuuv2/SLeT1aMmOBFt27TxdMu3/AFI49OxM67JqzhqLrI145/4ZhuqQ1Ztak1F8G17r7OjN7\nQNJaM/uGpM2SVjewnwCAOigZ9N39PknH5CnfptT9fQDAQYKMXAAICEEfAALCdIkAhiWyevsxXSIA\noCoEfQAICEEfAALCdIkAhqWOrtQUjPFpGuNTNy49+mRZc7P2zXmz7K57Zce9XbfceJUO7/6U5nz5\nLklM25gPV/oAEBCCPgAEhKAPAAEh6ANAQEjOAjBi1ZrgNVySu0jOAgBUhaAPAAEh6ANAQAj6ABAQ\ngj6AEaujK6FkZ7c6uhI6/Iu7M8+Tnd2acOe4zPP9z47N2jb9OBIR9AEgIAR9AAgIQR8AAkLQB4CA\nkJELAAUMlykbycgFAFSFoA8AASHoA0BAmC4RAAro6EoUnK7x5I8vV/Ptm9SzM5EpT645VpPvGqtp\nl6ema3zPlpd1+W2nDH7Hi+BKHwACQtAHgIAQ9AEgIAR9AAgIyVkA0ED1SPDaduZXSc4CAFSOoA8A\nASHoA0BACPoAEBAycgGggTq6Elr+8KP60IQXtbTzb3TzHddL6s/ujUtn97bddYh2LX4pkw3cVMf+\ncKUPAAEh6ANAQMoO+mbWZGabzWxdtDzHzDaYWa+ZXWNmYxrXTQBAPVRypb9S0tbY8kWSLnb3uZKe\nl7S8nh0DANRfWRm5ZtYuaY2kb0o6R9L7JT0t6c3u3mdmiyV93d2XFKtnbMdMn7HyHI1qe1UHdrVk\nHiVlPU8rtD5329x1kjL1p58XaqNYW7mZdKWmPSunX/mU079y+pzbx3xl8f0KrS+3L+W8JvHjLrS+\nktek1P6lXoN8fcqtq1RbpcZtNSo5jnhZ+hgqqa+S163Y8ZY6H4XGZDnv1XLayqfccVftfpW+TvmW\nc+vO149cQzFd4vckfUnSgWj5DZJecPe+aHm7pJn16BAAoHFKBn0zO03SbnffVE0DZrbCzDaa2cYD\nL++tpgoAQJ2U8z39EyV9wMyWSmqRdIikSyRNNrPm6Gq/XdKOfDu7+ypJq6TU7Z269BoAUJWSV/ru\nfr67t7v7bEkfkfQrd/+opPWSzog2WybpxlJ1vX3is5Kkud9K3dc6/EtPZ9YlO7sHbP/zxVfkXZ+7\nbUdXImtdS+vrmfL4tu+e+1DBvsW3a+ptzdp/8ZxtWW0U0vdK9u/Qf3tH/ylJdnaroyuhWaubNGbL\nOCU7uzVpfWtWu60bx5VsIy7ep46uxIA+5iuL75fs7Na8Tz+at+63zXyyZPvt057P25d03ZJ02Hf2\n5e1Hen2x83rIxFf6n/+6Ne/+uWNh80mXZy1P/E3/fvHjjvepo+2ZrLpGPZrd1vRrx2Ttnzve6iG3\nntzllsTAsZHs7Nbh5z9bVn2F+tzx/QN5t+voSqjlD+MG7JvveaH1fS+M0YTx2Z9l5fZj/3NjM89z\nx0LfS2OytpWkE2Y/pmLSdbRsHqf3zd8y4DzEY0pcfDxNvaVVb535VN56873Hii3nex2KjZl6jadi\navme/rmSzjGzXqXu8a+uT5cAAI1S0b9hcPc7JN0RPd8m6fj6dwkA0Chk5AJAQAj6ABCQYTddYr6E\nqtzlQskV5WwX3za+fal9Cu2fr45y+1FO3em6Kkk6K7S+VFJOvP58SWXFEpgKtVPu+nKSr8pN0MrX\nbr5jq0Y1bebuF5d7TnP3KbRdsUStUolG8f5Uc2zFjrnUe65UIlmpRLlS7RRan+/9E1dOW+WO60Jj\nN378lb6HhyI5CwAwAhD0ASAgBH0ACAhBHwACMuyCfm7GWjqjM23uRa/l3e+QO7KzW4tleqbbKJU9\nV0jfy6PV9+dmdXQl9C/HrhtQd6EMxXKk+xV/jJfnayO33/P++U9Z66fdlPowaP27flBwn6wM1Zy6\n563cMSADNt+5WnH0/w3YP7eNQvvmOuw7+wbUtXpR9n7pelo3Fc9k7uhKqO36VObn/qdr+4+Yb7ly\n1IB+FWpz9g+zl9OPJ8x+TMnO7gHnctL61qx6t/zVlfrliZeqbepLkqTf/uWlWdvHxV+jN/3X2Exb\nyc7urKzk3P7Ezf326wXrL0excxI/3nzr0pofzM4CnvWm5ypqp9D6ePsdXQm1r2keEAdO6ugt2rdC\n7bb/ZHTWtnMvfHXA9p9fcJvm/mMyszzv6y9nrd///NisOufP2FVx7CjXsAv6AIDGIegDQEAI+gAQ\nEII+AARk0DNy2799VsEsxFLZb8UyRYsplpFZTvZqoW2LlVcypV4lU6hVM61eOcdV6VR65WQWlttW\nJVPclfMaVDo2CmVTxusolcGaK19/y80uLZXZWuy9UklWbKFM4GLHWOg9W+6YyLe+UJuljjmfUhm/\nlSj0nsk3voq9D8t9TYod57Yzv0pGLgCgcgR9AAgIQR8AAjLoQT83QSeegPTGdf33vT4w/74B+65d\nvCqrnrEt+8pKYIgnYOSblm7mVaML9q9QPbnHlDud38fednfWfqX6V+4UapVOz5ZbXui40kY90pq3\nvFAduX3/xTsvyzzPl0j3H8ddm3n+9Xf8d1Z99lirDuxuKXpMcz/5SOEpILen+n7IHa0a/cA4zbx6\n9IDtcuUeU/z1P+zi/UWndYz3Pz2OOw9LasqkvVn9Su8779/2Dqjji8fcmv9YSvS3YALg9oGJWPF9\n8yX1JTu71fxw/gS3eFJZvO10+cyfjh7Qh3F3jyuaoFjofZQ275t/ztuHeHLTsqN+r46uhG498Qfy\nnS1FpyGsNtmsddxrefdPt3X4V/qnC537Dw9nbeNumeezLzMVkuzs1qTbU6/Zm6/rnzoy3la9k7S4\n0geAgBD0ASAgBH0ACAhBHwACMqjJWWb2sqSHBq3B4e2Nkp4Z6k4ME5yLfpyLfpyLfvPdfWI9Kmqu\nRyUVeKheWWUHOzPbyLlI4Vz041z041z0M7ON9aqL2zsAEBCCPgAEZLCD/qrSmwSDc9GPc9GPc9GP\nc9GvbudiUD/IBQAMLW7vAEBABiXom9mpZvaQmfWa2XmD0eZQMrNDzWy9mT1gZveb2cqofKqZ3WZm\nyehxSlRuZvb96PzcZ2bHDu0R1J+ZNZnZZjNbFy3PMbMN0TFfY2ZjovKx0XJvtH72UPa73sxsspld\nZ2YPmtlWM1sc6rgws89F748tZvYzM2sJZVyY2Y/NbLeZbYmVVTwOzGxZtH3SzJaV03bDg76ZNUm6\nVNJfSzpS0plmdmSj2x1ifZI+7+5HSlok6Z+iYz5P0u3uPk/S7dGylDo386KfFZIuG1jlQW+lpK2x\n5YskXezucyU9L2l5VL5c0vNR+cXRdiPJJZJucfcjJP2FUuckuHFhZjMlfVbSQnc/SlKTpI8onHHR\nLenUnLKKxoGZTZX0NUknSDpe0tfSvyiKcveG/khaLKkntny+pPMb3e5w+pF0o6RTlEpMmx6VTVcq\nb0GSrpB0Zmz7zHYj4UdSezSI3y1pnSRTKummOXeMSOqRtDh63hxtZ0N9DHU6D5MkPZp7PCGOC0kz\nJT0haWr0Oq+TtCSkcSFptqQt1Y4DSWdKuiJWnrVdoZ/BuL2TfnHTtkdlQYj+DD1G0gZJbe7+ZLTq\nKUlt0fORfo6+J+lLkg5Ey2+Q9IK790XL8ePNnIto/YvR9iPBHElPS/rP6FbXj8xsvAIcF+6+Q9K/\nS/qTpCeVep03KcxxkVbpOKhqfPBBbgOZ2QRJ10s6291fiq/z1K/mEf/VKTM7TdJud9801H0ZBpol\nHSvpMnc/RtJe9f8JLymocTFF0ulK/SKcIWm8Bt7uCFYjx8FgBP0dkg6NLbdHZSOamY1WKuBf7e43\nRMW7zGx6tH66pN1R+Ug+RydK+oCZPSZprVK3eC6RNNnM0v8GJH68mXMRrZ8k6dnB7HADbZe03d03\nRMvXKfVLIMRx8R5Jj7r70+6+T9INSo2VEMdFWqXjoKrxMRhB/x5J86JP5cco9WHNTYPQ7pAxM5O0\nWtJWd/9ubNVNktKfsC9T6l5/uvxvo0/pF0l6MfZn3kHN3c9393Z3n63Ua/8rd/+opPWSzog2yz0X\n6XN0RrT9iLjydfenJD1hZvOjopMlPaAAx4VSt3UWmdm46P2SPhfBjYuYSsdBj6T3mtmU6C+n90Zl\nxQ3SBxZLJT0s6RFJXxnqD1AG4XjfpdSfZvdJSkQ/S5W6B3m7pKSk/5U0NdrelPqG0yOS/qjUNxqG\n/DgacF46Ja2LnndIultSr6SfSxoblbdEy73R+o6h7nedz8ECSRujsfELSVNCHReSLpD0oKQtkq6S\nNDaUcSHpZ0p9lrFPqb8Al1czDiT9fXROeiX9XTltk5ELAAHhg1wACAhBHwACQtAHgIAQ9AEgIAR9\nAAgIQR8AAkLQB4CAEPQBICD/D6dLkr+Yz5X4AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkd2LdwKLWtP",
        "colab_type": "text"
      },
      "source": [
        "### Estimated Random Effects matrix\n",
        "\n",
        "The below reads in the Random effects variance predicted by `R`'s `lmer`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOqQRAROqdkv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read in estimated variance\n",
        "RFXVar_REst =pd.read_csv('/Data/BLMM-testdata/estd_rfxvar.csv',header=None).values\n",
        "\n",
        "# The RFX variance was stored in a strange way so we need to reformat slightly\n",
        "RFXVar_REst = spmatrix(RFXVar_REst[RFXVar_REst!=0],[0,0,1,1,2,2,3,3],[0,1,0,1,2,3,2,3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKE_Rr53qeHx",
        "colab_type": "text"
      },
      "source": [
        "### Estimated Theta\n",
        "\n",
        "The below calculates and outputs the `theta` parameter values which `lmer` estimated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeaYoXU0Lnlr",
        "colab_type": "code",
        "outputId": "0c4d2905-6095-445a-8d08-190b341f4767",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# We now need the Sparse Cholesky decomposition of the RFX variance to obtain \n",
        "# the theta values.\n",
        "f = sparse_chol(RFXVar_REst)\n",
        "\n",
        "# We map the L matrix from the decomposition to theta\n",
        "theta_REst = inv_mapping(f['L'])\n",
        "\n",
        "# This is the desired outcome/theta value of the PLS algorithm. It should match \n",
        "# the R output.\n",
        "print(theta_REst)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1.11365482  0.32356815  2.22872418  4.54822071 -0.17500295  0.42370817]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RStOLwF_LlTE",
        "colab_type": "text"
      },
      "source": [
        "### Y vector\n",
        "\n",
        "The response vector is read in here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DG8eWNdpPQOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y=matrix(pd.read_csv('/Data/BLMM-testdata/Y.csv',header=None).values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHm6DjdbPTvg",
        "colab_type": "text"
      },
      "source": [
        "### X matrix\n",
        "\n",
        "The fixed effects design matrix is read in here. It consists of an intercept and two random (Gaussian) columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqTKH4n_Po8e",
        "colab_type": "code",
        "outputId": "900913cd-752e-406a-aec8-0f8473394cb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "X=matrix(pd.read_csv('/Data/BLMM-testdata/X.csv',header=None).values)\n",
        "\n",
        "# Image of the first 20 rows of X\n",
        "imshow(X[1:20,:])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fa0ea9746a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFYAAAD8CAYAAADt0VN/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADBNJREFUeJztnXuMXVUVh78fbXnVhjeUIm9Kk0Kw\nkKaCqCkWEUZClaC2MVAERVQUjGhQIxiMRkOUaCCQSsvDFGjkZYXyqECCGEBK00JLqRQCgQIFWuiD\nAnXo8o+zBy+393b23HNXe+/p+pLJnHvOuufs+WbPvuexZm2ZGUH72WZLN6CqhFgnQqwTIdaJEOtE\niHUixDoRYp0IsU4M3tINaMQOu2xnO40Ymh2/z+B3smMXvr1Hdmzvirf4YO07yn5DDR0pdqcRQzn9\nxgnZ8b/Zc1527CF/Pzc79rVf/zE7tp5SQ4GkEyUtkbRU0kUNtm8naWba/pikA8ocr5toWaykQcCV\nwEnAaGCypNF1YWcDb5nZIcDlwO9aPV63UabHjgOWmtnzZrYeuBmYWBczEbg+Ld8CTJDU0pjVbZQR\nuw/wUs3rl9O6hjFm1gusAnYrccyuoWNOtySdI2mupLnr3np/SzenNGXELgP2rXn98bSuYYykwcBO\nwIpGOzOzqWY21szG7rjLdiWa1RmUEfs4MFLSgZK2BSYBs+piZgFT0vJpwAO2lTyyaPk81sx6JZ0H\n3AsMAqab2SJJlwJzzWwWMA34i6SlwEoK+VsFpS4QzGw2MLtu3cU1y+8BXylzjG6lI6+8BkrPPkdl\nx5427/Hs2BlD17XSHKCDzgqqRoh1IsQ6EWKdCLFOhFgnQqwTIdaJEOtEiHUixDpRiXsFM176V3bs\ncX/8cXbsqpWPtNIcIHqsGyHWiRDrRIh1IsQ6EWKdCLFOlMnd2lfSg5KelrRI0vkNYsZLWiVpfvq6\nuNG+qkiZC4Re4EdmNk/SMOAJSXPM7Om6uH+a2ckljtOVtNxjzexVM5uXltcAi9k4d2urpS2XtCnv\n9UjgsQabj5G0AHgFuNDMFrXjmLX0/OLC7Nj3R+Un4lgJO6XFSvoYcCtwgZmtrts8D9jfzNZK6gHu\nAEY22c85wDkAw/besWyztjhlM7qHUEidYWa31W83s9VmtjYtzwaGSNq90b4iKS6REoinAYvN7A9N\nYob3JRpLGpeO1zDbsGqUGQqOBU4HnpI0P637GbAfgJldTZFh+B1JvcC7wKTINuwHM3sY2GTau5ld\nAVzR6jG6mbjyciLEOhFinQixToRYJ0KsE5V4/P2Z7ze6RdGYOx4alx1rJbpd9FgnQqwTIdaJEOtE\niHUixDoRYp0IsU6EWCdCrBOVuKS955ajs2M3HPTf/B1v0/pTpOixToRYJ0qLlfSCpKdS0tvcBtsl\n6U+pWtyTkvKrNnQx7RpjjzOzN5tsO4ki+2Uk8EngqvS90myOoWAicIMVPArsLGnvzXDcLUo7xBpw\nn6QnUv5VPTkV5SpX0KwdQ8GnzWyZpD2BOZKeMbOHBroTM5sKTAUYftiuXZ8tU7rHmtmy9P114HaK\nYpK15FSUqxxlsw2HpmxuJA0FTgAW1oXNAs5IZwdHA6vM7NUyx+0Gyg4FewG3p4TCwcCNZnaPpHPh\nw8S42UAPsBRYB3yj5DG7grKV4p4HPtFg/dU1ywZ8r8xxupFK3CtYd2j+WcQZRz2aHTtt6NpWmgPE\nJa0bIdaJEOtEiHUixDoRYp0IsU6EWCdCrBMh1olKXNIOWb5tduwNjxybHbti7UaP8LKJHutEiHUi\nxDoRYp0IsU6EWCdCrBNlSpeMqilUNl/SakkX1MVEQbOBYmZLgDHw4YyfyyjyCuqJgmYlmAA8Z2Yv\ntml/XU+7xE4Cbmqy7RhJCyTdLemwNh2v42lHQbNtgVOAnzbYvFkKmvWOyH/8PeiN/PsKbGh9Ct12\n9NiTgHlmtrx+QxQ0K8dkmgwDUdCsRVIi3OeBb9esq83bioJmrWBm71A35XRd3lYUNAvaS4h1IsQ6\nEWKdCLFOhFgnKvH4+67P5p/RrdywfXbst659vZXmANFj3QixToRYJ0KsEyHWiRDrRIh1IsQ6EWKd\nCLFOhFgnKnGvoOf+H2THnnREfZ2K5qzond1Kc4DosW5kiZU0XdLrkhbWrNtV0hxJz6bvuzR575QU\n86ykKe1qeKeT22OvA06sW3cRcL+ZjQTuT68/gqRdgUsoCpiNAy5p9guoGlliU7mnlXWrJwLXp+Xr\ngS81eOsXgDlmttLM3gLmsPEvqJKUGWP3qqlG9BpF4Z16soqZVZG2fHil7JZSGS5VqxRXRuzyvhqF\n6Xuj5xjZxcwiKe7/zAL6PuWnAH9rEHMvcIKkXdKH1glpXeXJPd26CXgEGCXpZUlnA78FPi/pWeD4\n9BpJYyVdA2BmK4FfAY+nr0vTusqTdeVlZpObbJrQIHYu8M2a19OB6S21roupxCXtoLfzf4yHXjo4\nO3bt+tbH+rikdSLEOhFinQixToRYJ0KsEyHWiRDrRIh1IsQ6EWKdqMS9gtFjX8iOXfjk/tmxG9YP\naqE1BdFjnQixToRYJ0KsEyHWiRDrRL9im+RtXSbpmTS55O2Sdm7y3k1OVFllcnrsdWycFjQHONzM\njgD+Q+MKRn0cZ2ZjzGxsa03sTvoV2yhvy8zuM7Pe9PJRikSMoIZ2jLFnAXc32dbfRJWVpWwVo58D\nvcCMJiHZE1WWKWj21OL9smMPHZ0/XePKHdYPqB21lKnGeSZwMvD1ZiWfMiaqrI2N3C1JJwI/AU4x\ns3VNYnImqqwsOadbjfK2rgCGUfx5z5d0dYodIanvPyL2Ah6WtAD4N3CXmd3j8lN0IP2OsU3ytqY1\niX2FYkbPphNVbi3ElZcTIdaJEOtEiHUixDoRYp0IsU5U4vH3zgvzf4zn1uzbf1Di/XUDKJReR/RY\nJ0KsEyHWiRDrRIh1IsQ6EWKdCLFOhFgnQqwTlbikHfRuftWUH37xzuzYy65d1UpzgOixbrSaFPdL\nSctqZu/safLeEyUtkbRU0kZ1uapMq0lxAJenZLcxaXa5j5Bm/rySYoa60cBkSaPLNLabaCkpLpNx\nwFIze97M1gM3UxRB2yooM8ael/Jjpzcpq7fVFjOD1sVeBRxMMeHvq8DvyzYkCpoBZrbczD4wsw3A\nn2mc7JZdzCztM5Li+irEJb5M42S3x4GRkg5Mc9dOoiiCtlXQ7wVCSoobD+wu6WWKsqXjJY2hSCx+\ngTTLp6QRwDVm1mNmvZLOo6gMNwiYbmaLXH6KDsQtKS69ng20Xo+5i4krLycqca9gzYH5sUvWDc+O\nfW/DkBZaUxA91okQ60SIdSLEOhFinQixToRYJ0KsEyHWiRDrRCUuaZecdVV27EH/OCs7dtV7Df9R\nPYvosU6EWCdCrBMh1okQ60SIdSLEOpHzlHY6RVGd183s8LRuJjAqhewMvG1mYxq89wVgDfAB0Ls1\nFTXLuUC4jqIGzA19K8zsa33Lkn4PbCqR9Dgze7PVBnYrOY+/H5J0QKNtkgR8Ffhce5vV/ZQdYz8D\nLDezZ5tsz64UV7XcrbL3CiYDN21ie3alODObCkwFGH7YrgOaMfTYJ0/ND149gEfaH2ggzfgIZSrF\nDQZOBWY2ixlIpbiqUWYoOB54xsxebrQxKsX1Q5NKcVBkD95UFxuV4hKtJsVhZmc2WBeV4hJx5eVE\niHUixDoRYp0IsU6EWCfUpLz2FkXSG8CLdat3Bzb3XbJRZjaslTd2ZF6Bme1Rv07S3M19P7fMbCMx\nFDgRYp3oJrFTu+mYHfnhVQW6qcd2FR0ntr9yJ5K2kzQzbX+s2fO4ARxvX0kPSnpa0iJJ5zeIGS9p\nVU2plov73bGZdcwXxT8zPwccBGwLLABG18V8F7g6LU8CZpY85t7AUWl5GMX8ZPXHHA/cOZD9dlqP\nzSl3MhG4Pi3fAkxIT4tbwsxeNbN5aXkNsJg2VALpNLE55U4+jLFisrZVwG7tOHgaVo4EHmuw+RhJ\nCyTdLemw/vbVkVdeWwJJHwNuBS4ws9V1m+cB+5vZ2lQK6w5g5Kb212k9NqfcyYcx6UnxTsCKMgeV\nNIRC6gwzu61+u5mtNrO1aXk2METS7pvaZ6eJzSl3MguYkpZPAx6wEifjaXyeBiw2sz80iRneN45L\nGkfhbdO/zC19JtDgU7qH4pP5OeDnad2lFJOyAWwP/BVYSvH096CSx/s0RcbOk8D89NUDnAucm2LO\nAxZRnKU8Cnyqv/3GlZcTnTYUVIYQ60SIdSLEOhFinQixToRYJ0KsE/8D9NqNWg2rPVoAAAAASUVO\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_sXGD8qzskp",
        "colab_type": "text"
      },
      "source": [
        "### Number of Levels and Parameters\n",
        "\n",
        "The number of levels is given by a vector with one entry for each grouping factor. e.g. nlevels=[10,2] means there are 10 levels for factor 1 and 2 levels for factor 2. \n",
        "\n",
        "The number of parameters is given by a vector with one entry for each grouping factor. e.g. nparams=[3,4] means there are 3 variables for factor 1 and 4 variables for factor 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e5bUA2DztCf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlevels = np.array([20,3])\n",
        "nparams = np.array([2,2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0byKXygpCa1P",
        "colab_type": "text"
      },
      "source": [
        "### True b values\n",
        "\n",
        "The true recorded values of the random effects b vector in this example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzwuuWSdCbCJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b_True=matrix(pd.read_csv('/Data/BLMM-testdata/true_b.csv',header=None).values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bvfns6c-CbPd",
        "colab_type": "text"
      },
      "source": [
        "### True beta values\n",
        "\n",
        "The true fixed effects parameters used to generate this example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlvolWwvCbbP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "beta_True=matrix(pd.read_csv('/Data/BLMM-testdata/true_beta.csv',header=None).values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjWoiZtYyuRh",
        "colab_type": "text"
      },
      "source": [
        "### Product Matrices\n",
        "\n",
        "It is useful to calculate all product of matrices before hand as it is both more computationally efficient and also similar to the setting we are interested in. In `cvxopt`, transposing matrices is surprisingly costly in terms of time so even transposes are calculated here to save them having to be recalculated during each iteration of the minimisation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMX_8BRYzFhN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Z tranpose X\n",
        "ZtX=cvxopt.spmatrix.trans(Z)*X\n",
        "\n",
        "# Z tranpose Y\n",
        "ZtY=cvxopt.spmatrix.trans(Z)*Y\n",
        "\n",
        "# X tranpose X\n",
        "XtX=cvxopt.matrix.trans(X)*X\n",
        "\n",
        "# Z tranpose Z\n",
        "ZtZ=cvxopt.spmatrix.trans(Z)*Z\n",
        "\n",
        "# X tranpose Y\n",
        "XtY=cvxopt.matrix.trans(X)*Y\n",
        "\n",
        "# Y tranpose X\n",
        "YtX=cvxopt.matrix.trans(Y)*X\n",
        "\n",
        "# Y transpose Z\n",
        "YtZ=cvxopt.matrix.trans(Y)*Z\n",
        "\n",
        "# X tranpose Z\n",
        "XtZ=cvxopt.matrix.trans(X)*Z\n",
        "\n",
        "# Y tranpose Y\n",
        "YtY=cvxopt.matrix.trans(Y)*Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhOHZ6F2DU5e",
        "colab_type": "text"
      },
      "source": [
        "## Calculating the Minimised Log-Likelihood\n",
        "\n",
        "To calculate the minimised log-likelihood the method given by Bates (2015) is used. This involves minimising for a parameter vector theta, a penalized least squares function which is based on the sparse cholesky decomposition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVtC2_vLtsY2",
        "colab_type": "text"
      },
      "source": [
        "### PLS function\n",
        "\n",
        "This function calculates the log likelihood value for parameter vector theta using **P**enalized **L**east **S**quares..\n",
        "\n",
        "---\n",
        "\n",
        "The following inputs are required for this function:\n",
        "\n",
        "---\n",
        "\n",
        " - **theta**: The parameter estimate.\n",
        " - **ZtX**: Z transpose multiplied by X.\n",
        " - **ZtY**: Z transpose multiplied by Y.\n",
        " - **XtX**: X transpose multiplied by X.\n",
        " - **ZtZ**: Z transpose multiplied by Z.\n",
        " - **XtY**: X transpose multiplied by Y.\n",
        " - **YtX**: Y transpose multiplied by X.\n",
        " - **YtZ**: Y transpose multiplied by Z.\n",
        " - **XtZ**: X transpose multiplied by Z.\n",
        " - **YtY**: Y transpose multiplied by Y.\n",
        " - **P**: The sparse permutation for Lamda'Z'ZLambda+I\n",
        " - **tinds**: A vector specifying how many times each theta parameter should be repeated. For example, if theta=[0.1,0.8,0.3] and theta_inds=[1,1,1,2,3,3], then the values to be mapped into the sparse matrix would be [0.1,0.1,0.1,0.8,0.3,0.3].\n",
        " - **r_inds**: The row indices of the elements mapped into the sparse matrix.\n",
        " - **c_inds**: The column indices of the elements mapped into the sparse matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PR3bUshSts74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def PLS(theta, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, P, tinds, rinds, cinds):\n",
        "\n",
        "    # Obtain Lambda\n",
        "    Lambda = mapping(theta, tinds, rinds, cinds)\n",
        "    \n",
        "    # Obtain Lambda'\n",
        "    Lambdat = spmatrix.trans(Lambda)\n",
        "\n",
        "    # Obtain Lambda'Z'Y and Lambda'Z'X\n",
        "    LambdatZtY = Lambdat*ZtY\n",
        "    LambdatZtX = Lambdat*ZtX\n",
        "    \n",
        "    # Set the factorisation to use LL' instead of LDL'\n",
        "    cholmod.options['supernodal']=2\n",
        "\n",
        "    # Obtain the cholesky decomposition\n",
        "    LambdatZtZLambda = Lambdat*ZtZ*Lambda\n",
        "    I = spmatrix(1.0, range(Lambda.size[0]), range(Lambda.size[0]))\n",
        "    chol_dict = sparse_chol(LambdatZtZLambda+I, perm=P, retF=True, retP=False, retL=False)\n",
        "    F = chol_dict['F']\n",
        "\n",
        "    # Obtain C_u (annoyingly solve writes over the second argument,\n",
        "    # whereas spsolve outputs)\n",
        "    Cu = LambdatZtY[P,:]\n",
        "    cholmod.solve(F,Cu,sys=4)\n",
        "\n",
        "    # Obtain RZX\n",
        "    RZX = LambdatZtX[P,:]\n",
        "    cholmod.solve(F,RZX,sys=4)\n",
        "\n",
        "    # Obtain RXtRX\n",
        "    RXtRX = XtX - matrix.trans(RZX)*RZX\n",
        "\n",
        "    # Obtain beta estimates (note: gesv also replaces the second\n",
        "    # argument)\n",
        "    betahat = XtY - matrix.trans(RZX)*Cu\n",
        "    lapack.posv(RXtRX, betahat)\n",
        "\n",
        "    # Obtain u estimates\n",
        "    uhat = Cu-RZX*betahat\n",
        "    cholmod.solve(F,uhat,sys=5)\n",
        "    cholmod.solve(F,uhat,sys=8)\n",
        "\n",
        "    # Obtain b estimates\n",
        "    bhat = Lambda*uhat\n",
        "\n",
        "    # Obtain residuals sum of squares\n",
        "    resss = YtY-2*YtX*betahat-2*YtZ*bhat+2*matrix.trans(betahat)*XtZ*bhat+matrix.trans(betahat)*XtX*betahat+matrix.trans(bhat)*ZtZ*bhat\n",
        "\n",
        "    # Obtain penalised residual sum of squares\n",
        "    pss = resss + matrix.trans(uhat)*uhat\n",
        "\n",
        "    # Obtain Log(|L|^2)\n",
        "    logdet = 2*sum(cvxopt.log(cholmod.diag(F)))\n",
        "\n",
        "    # Obtain log likelihood\n",
        "    logllh = -logdet/2-X.size[0]/2*(1+np.log(2*np.pi*pss)-np.log(X.size[0]))\n",
        "\n",
        "    return(-logllh[0,0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzkEDjxsxHCl",
        "colab_type": "text"
      },
      "source": [
        "## Minimising the Log-Likelihood\n",
        "\n",
        "To minimise the log-likelihood  we use the `scipy optimize` functions on the PLS algorithm. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwPiugXwyFpW",
        "colab_type": "code",
        "outputId": "d91bf13a-2cda-4cad-b59e-f6e0498d874d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        }
      },
      "source": [
        "# Initial theta value. Bates (2005) suggests using vec(I) where I is the identity matrix\n",
        "theta0=np.array([1,0,1,1,0,1])\n",
        "\n",
        "# Obtain a random Lambda matrix with the correct sparsity for the permutation vector\n",
        "tinds,rinds,cinds=get_mapping(nlevels, nparams)\n",
        "Lam=mapping(np.random.randn(6),tinds,rinds,cinds)\n",
        "\n",
        "# Obtain Lambda'Z'ZLambda\n",
        "LamtZt = spmatrix.trans(Lam)*spmatrix.trans(Z)\n",
        "LamtZtZLam = LamtZt*spmatrix.trans(LamtZt)\n",
        "\n",
        "# Obtaining permutation for PLS\n",
        "P=cvxopt.amd.order(LamtZtZLam)\n",
        "\n",
        "# Obtain estimate using `Nelder-Mead`\n",
        "theta_Est_nm=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY ,P,tinds, rinds, cinds), method='Nelder-Mead', tol=1e-6).x\n",
        "\n",
        "# Obtain estimate using `Powell`\n",
        "theta_Est_pow=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY ,P,tinds, rinds, cinds), method='Powell', tol=1e-6).x\n",
        "\n",
        "# Obtain estimate using `CG`\n",
        "theta_Est_cg=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY ,P,tinds, rinds, cinds), method='CG', tol=1e-6).x\n",
        "\n",
        "# Obtain estimate using `BFGS`\n",
        "theta_Est_bfgs=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY ,P,tinds, rinds, cinds), method='BFGS', tol=1e-6).x\n",
        "\n",
        "# Obtain estimate using `L-BFGS-B`\n",
        "theta_Est_lbfgsb=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY ,P,tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "\n",
        "# Obtain estimate using `TNC`\n",
        "theta_Est_tnc=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY ,P,tinds, rinds, cinds), method='TNC', tol=1e-6).x\n",
        "\n",
        "# Obtain estimate using `COBYLA`\n",
        "theta_Est_cobyla=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY ,P,tinds, rinds, cinds), method='COBYLA', tol=1e-6).x\n",
        "\n",
        "# Obtain estimate using `SLSQP`\n",
        "theta_Est_slsqp=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY ,P,tinds, rinds, cinds), method='SLSQP', tol=1e-6).x\n",
        "\n",
        "# Obtain estimate using `trust-constr`\n",
        "theta_Est_trcon=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY ,P,tinds, rinds, cinds), method='trust-constr', tol=1e-6).x\n",
        "\n",
        "# Print lmer estimate\n",
        "print('Lmer Estimate')\n",
        "print(theta_REst)\n",
        "\n",
        "# Print estimates\n",
        "print('Nelder-Mead Python Estimate')\n",
        "print(theta_Est_nm)\n",
        "print('Powell Python Estimate')\n",
        "print(theta_Est_pow)\n",
        "print('CG Python Estimate')\n",
        "print(theta_Est_cg)\n",
        "print('BFGS Python Estimate')\n",
        "print(theta_Est_bfgs)\n",
        "print('L-BFGS-B Python Estimate')\n",
        "print(theta_Est_lbfgsb)\n",
        "print('TNC Python Estimate')\n",
        "print(theta_Est_tnc)\n",
        "print('COBYLA Python Estimate')\n",
        "print(theta_Est_cobyla)\n",
        "print('SLSQP Python Estimate')\n",
        "print(theta_Est_slsqp)\n",
        "print('trust-constr Python Estimate')\n",
        "print(theta_Est_trcon)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lmer Estimate\n",
            "[ 1.11365482  0.32356815  2.22872418  4.54822071 -0.17500295  0.42370817]\n",
            "Nelder-Mead Python Estimate\n",
            "[ 1.11126692  0.32241667  2.22404676  4.53974109 -0.1746667   0.42288782]\n",
            "Powell Python Estimate\n",
            "[ 1.1112632   0.32241717  2.22404931  4.53966449 -0.17465885 -0.42289267]\n",
            "CG Python Estimate\n",
            "[ 1.11187291  0.32476648  2.22266278  4.41336705 -0.16554107  0.4247196 ]\n",
            "BFGS Python Estimate\n",
            "[ 1.11169947  0.32258166  2.22494603  4.57612851 -0.17720115  0.42339598]\n",
            "L-BFGS-B Python Estimate\n",
            "[ 1.11343291  0.3213067   2.23080559  4.54830593 -0.16751323  0.42602596]\n",
            "TNC Python Estimate\n",
            "[ 1.10623504  0.16487256  2.25091699  4.56264169 -0.15950864 -0.42704138]\n",
            "COBYLA Python Estimate\n",
            "[ 1.11116203  0.32251513  2.22404271  4.52306875 -0.17353927  0.42305785]\n",
            "SLSQP Python Estimate\n",
            "[ 1.1115014   0.31759479  2.22422635  4.42822417 -0.16586302 -0.42316456]\n",
            "trust-constr Python Estimate\n",
            "[ 1.11130388  0.32632481  2.22576667  4.54084609 -0.17259623  0.42390698]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxNYlQkQ_asL",
        "colab_type": "text"
      },
      "source": [
        "### Time efficiency\n",
        "\n",
        "The below runs each method 100 times and works out the average time taken to produce the estimates. In this example, it is clear that `Powell` performs much quicker than `Nelder-Mead`. From now on, in the following sections `Powell` is the default method used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYq_1-3U_nMn",
        "colab_type": "code",
        "outputId": "f6effde8-3a4e-4e2d-b1c5-ba9c708e0eec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "source": [
        "# Average time\n",
        "time_nm = 0;\n",
        "time_p = 0;\n",
        "time_cg = 0;\n",
        "time_bfgs = 0;\n",
        "time_lbfgsb = 0;\n",
        "time_tnc = 0;\n",
        "time_cobyla = 0;\n",
        "time_slsqp = 0;\n",
        "time_trcon = 0;\n",
        "\n",
        "# Number of iterations\n",
        "n = 100;\n",
        "\n",
        "for i in range(n):\n",
        "  \n",
        "  # Obtain estimate using `Nelder-Mead`\n",
        "  t1 = time.time()\n",
        "  theta_Est_nm=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY ,P,tinds, rinds, cinds), method='Nelder-Mead', tol=1e-6).x\n",
        "  t2 = time.time()\n",
        "  time_nm = time_nm + t2 - t1\n",
        "  \n",
        "  # Obtain estimate using `Powell`\n",
        "  t1 = time.time()\n",
        "  theta_Est_p=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY ,P,tinds, rinds, cinds), method='Powell', tol=1e-6).x\n",
        "  t2 = time.time()\n",
        "  time_p = time_p + t2 - t1\n",
        "  \n",
        "  # Obtain estimate using `CG`\n",
        "  t1 = time.time()\n",
        "  theta_Est_cg=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY ,P,tinds, rinds, cinds), method='CG', tol=1e-6).x\n",
        "  t2 = time.time()\n",
        "  time_cg = time_cg + t2 - t1\n",
        "  \n",
        "  # Obtain estimate using `BFGS`\n",
        "  t1 = time.time()\n",
        "  theta_Est_bfgs=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY ,P,tinds, rinds, cinds), method='BFGS', tol=1e-6).x\n",
        "  t2 = time.time()\n",
        "  time_bfgs = time_bfgs + t2 - t1\n",
        "  \n",
        "  # Obtain estimate using `L-BFGS-B`\n",
        "  t1 = time.time()\n",
        "  theta_Est_lbfgsb=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY ,P,tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "  t2 = time.time()\n",
        "  time_lbfgsb = time_lbfgsb + t2 - t1\n",
        "\n",
        "  # Obtain estimate using `TNC`\n",
        "  t1 = time.time()\n",
        "  theta_Est_tnc=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY ,P,tinds, rinds, cinds), method='TNC', tol=1e-6).x\n",
        "  t2 = time.time()\n",
        "  time_tnc = time_tnc + t2 - t1\n",
        "  \n",
        "  # Obtain estimate using `COBYLA`\n",
        "  t1 = time.time()\n",
        "  theta_Est_cobyla=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY ,P,tinds, rinds, cinds), method='COBYLA', tol=1e-6).x\n",
        "  t2 = time.time()\n",
        "  time_cobyla = time_cobyla + t2 - t1\n",
        "  \n",
        "  # Obtain estimate using `SLSQP`\n",
        "  t1 = time.time()\n",
        "  theta_Est_slsqp=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY ,P,tinds, rinds, cinds), method='SLSQP', tol=1e-6).x\n",
        "  t2 = time.time()\n",
        "  time_slsqp = time_slsqp + t2 - t1\n",
        "\n",
        "  # Obtain estimate using `trust-constr`\n",
        "  t1 = time.time()\n",
        "  theta_Est_trcon=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY ,P,tinds, rinds, cinds), method='trust-constr', tol=1e-6).x\n",
        "  t2 = time.time()\n",
        "  time_trcon = time_trcon + t2 - t1\n",
        "\n",
        "  \n",
        "\n",
        "# Convert to averages\n",
        "time_nm = time_nm/n\n",
        "time_p = time_p/n\n",
        "time_cg = time_cg/n\n",
        "time_bfgs = time_bfgs/n\n",
        "time_lbfgsb = time_lbfgsb/n\n",
        "time_tnc = time_tnc/n\n",
        "time_cobyla = time_cobyla/n\n",
        "time_slsqp = time_slsqp/n\n",
        "time_trcon = time_trcon/n\n",
        "\n",
        "# Print\n",
        "print('Average Nelder-Mead estimation time:')\n",
        "print(time_nm)\n",
        "print('Average Powell estimation time:')\n",
        "print(time_p)\n",
        "print('Average CG estimation time:')\n",
        "print(time_cg)\n",
        "print('Average BFGS estimation time:')\n",
        "print(time_bfgs)\n",
        "print('Average L-BFGS-B estimation time:')\n",
        "print(time_lbfgsb)\n",
        "print('Average TNC estimation time:')\n",
        "print(time_tnc)\n",
        "print('Average COBYLA estimation time:')\n",
        "print(time_cobyla)\n",
        "print('Average SLSQP estimation time:')\n",
        "print(time_slsqp)\n",
        "print('Average trust-contstr estimation time:')\n",
        "print(time_trcon)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Nelder-Mead estimation time:\n",
            "0.4921038866043091\n",
            "Average Powell estimation time:\n",
            "0.19121604919433594\n",
            "Average CG estimation time:\n",
            "0.41337176561355593\n",
            "Average BFGS estimation time:\n",
            "0.35944594860076906\n",
            "Average L-BFGS-B estimation time:\n",
            "0.039960277080535886\n",
            "Average TNC estimation time:\n",
            "0.2796543264389038\n",
            "Average COBYLA estimation time:\n",
            "0.34834303855896\n",
            "Average SLSQP estimation time:\n",
            "0.055256614685058596\n",
            "Average trust-contstr estimation time:\n",
            "0.26811991930007933\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Nazpuh-A5vx",
        "colab_type": "text"
      },
      "source": [
        "## Scaling up the computation (Two voxels)\n",
        "\n",
        "This section has several implemented ideas for scaling up the computation to compute several similar models at once. For simplicity it is assumed here that X and Z are the same across voxels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enYnjHWXBVmO",
        "colab_type": "text"
      },
      "source": [
        "### Toy data for Neighbouring voxel\n",
        "\n",
        "To assess the potential of the following ideas a toy data example is created below. The idea behind this is that we wish to calculate both the model in the example used in the previous sections and, additionally a similar model from a neighbouring voxel (variables related to the neighbouring voxel will have postfix `_n`). \n",
        "\n",
        "This is not a rigourous test, but just a toy example to see if we can lower the computational time in an example vaguely similar to what we may expect in reality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzUVeNWeDLmy",
        "colab_type": "text"
      },
      "source": [
        "#### Beta vector\n",
        "\n",
        "In a neighbouring voxel, we would expect similar Beta values but not necessarily the same. To simulate this, I have added some normal noise with variance, 1/2, to the original beta values to obtain a new beta value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hKzWuueBbeI",
        "colab_type": "code",
        "outputId": "6fbd1a7d-616c-4f27-e8f2-b0614029d46b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "# Given a beta vector this function makes a beta \n",
        "# vector for the neighbouring voxel\n",
        "def beta_n(beta):\n",
        "  return(beta + cvxopt.normal(beta.size[0],1)/np.sqrt(2))\n",
        "\n",
        "# Example\n",
        "beta_True_n = beta_n(beta_True)\n",
        "  \n",
        "# print betas for comparison\n",
        "print(\"Beta for voxel 1\")\n",
        "print(beta_True)\n",
        "print(\"Beta for voxel 2\")\n",
        "print(beta_True_n)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Beta for voxel 1\n",
            "[ 1]\n",
            "[ 2]\n",
            "[ 3]\n",
            "\n",
            "Beta for voxel 2\n",
            "[ 1.09e+00]\n",
            "[ 1.94e+00]\n",
            "[ 4.18e+00]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTQFQbRLDKmq",
        "colab_type": "text"
      },
      "source": [
        "#### b vector\n",
        "\n",
        "In a neighbouring voxel, we may also expect similar b values. To simulate this, I have added some normal noise with variance, 1/2, to the original b values to obtain a new b value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsyjopBUFNt0",
        "colab_type": "code",
        "outputId": "42c72aed-176b-4d6a-8dd4-84b07349e78c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "# Given a b vector this function makes a b\n",
        "# vector for the neighbouring voxel\n",
        "def b_n(b):\n",
        "  return(b + cvxopt.normal(b.size[0],1)/np.sqrt(2))\n",
        "\n",
        "# Example\n",
        "b_True_n = b_n(b_True)\n",
        "  \n",
        "# print bs for comparison\n",
        "print(\"b for voxel 1 (first 5 elements)\")\n",
        "print(b_True[1:5])\n",
        "print(\"b for voxel 2 (first 5 elements)\")\n",
        "print(b_True_n[1:5])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b for voxel 1 (first 5 elements)\n",
            "[-4.23e+00]\n",
            "[ 1.44e+00]\n",
            "[ 1.41e+00]\n",
            "[ 1.35e+00]\n",
            "\n",
            "b for voxel 2 (first 5 elements)\n",
            "[-4.37e+00]\n",
            "[ 1.13e+00]\n",
            "[ 1.50e+00]\n",
            "[ 1.65e+00]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFb5eO8jBWSf",
        "colab_type": "text"
      },
      "source": [
        " #### Y vector (New response)\n",
        " \n",
        "I now generate a new response vector with the new beta and b values for the neighbouring voxel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8V59rDqEGU5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Neighbouring voxels response vector\n",
        "Y_n = X*beta_True_n+Z*b_True_n+cvxopt.normal(1000,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1fjCdxgH3aV",
        "colab_type": "text"
      },
      "source": [
        "### Product Matrices\n",
        "\n",
        "All products of matrices are calculated beforehand as it is both more computationally efficient and also similar to the setting we are interested in. For the neighbouring voxel only those involving the Y vector (response) need be recalculated as X and Z have not changed between voxel in this example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wnx7aj6OH3tA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Z tranpose Y_n\n",
        "ZtY_n=cvxopt.spmatrix.trans(Z)*Y_n\n",
        "\n",
        "# X tranpose Y_n\n",
        "XtY_n=cvxopt.matrix.trans(X)*Y_n\n",
        "\n",
        "# Y_n tranpose X\n",
        "YtX_n=cvxopt.matrix.trans(Y_n)*X\n",
        "\n",
        "# Y_n transpose Z\n",
        "YtZ_n=cvxopt.matrix.trans(Y_n)*Z\n",
        "\n",
        "# Y_n tranpose Y_n\n",
        "YtY_n=cvxopt.matrix.trans(Y_n)*Y_n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9CsWWXKGk-I",
        "colab_type": "text"
      },
      "source": [
        "### Idea 1: Initial Estimate\n",
        "\n",
        "The idea behind this is relatively simple. Once we have estimated the model for one voxel, we use the estimated theta vector from that voxel as the initial estimate for it's neighbours.\n",
        "\n",
        "However, it didn't work/no meaningful time improvement was observed in the toy example.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5c3WZF5HDm-",
        "colab_type": "code",
        "outputId": "38bb342d-2158-49ca-f169-2903ec1042cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# Estimate first voxel\n",
        "theta_Est=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY ,P,tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "\n",
        "# Estimate second voxel\n",
        "theta_Est_n=minimize(PLS, theta_Est, args=(ZtX, ZtY_n, XtX, ZtZ, XtY_n, YtX_n, YtZ_n, XtZ, YtY_n ,P,tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "\n",
        "# Print results\n",
        "print('Estimate for voxel 1')\n",
        "print(theta_Est)\n",
        "\n",
        "# Print results for neighbouring voxel\n",
        "print('Estimate for voxel 2')\n",
        "print(theta_Est_n)\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Estimate for voxel 1\n",
            "[ 1.11343291  0.3213067   2.23080559  4.54830593 -0.16751323  0.42602596]\n",
            "Estimate for voxel 2\n",
            "[ 1.21228134  0.24719727  2.13841449  3.2870374  -1.33832044  0.19605888]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqbOzGU-It3f",
        "colab_type": "text"
      },
      "source": [
        "#### Time efficiency\n",
        "\n",
        "What is really important here is not the estimate values (assuming they are correct), but the time taken to do this for both voxels. In the below, the two voxels are estimated 100 times twice, once reusing the estmate from the first voxel in estimating the second, and once computing the voxels completely seperately.\n",
        "\n",
        "**Conclusion:** This idea didn't really work... the time complexity was not meaningfully reduced (reduction of <0.01s in most runs)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OM3CTjNJXp_",
        "colab_type": "code",
        "outputId": "9281b2d5-eff8-4631-bf7e-114e46e78ba5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# Average time\n",
        "time_original = 0;\n",
        "time_idea1 = 0;\n",
        "\n",
        "# Number of iterations\n",
        "n = 100;\n",
        "\n",
        "for i in range(n):\n",
        "  \n",
        "  # New neigbouring voxel response\n",
        "  Y_n = X*beta_n(beta_True)+Z*b_n(b_True)+cvxopt.normal(1000,1)\n",
        "  \n",
        "  # Z tranpose Y_n\n",
        "  ZtY_n=cvxopt.spmatrix.trans(Z)*Y_n\n",
        "\n",
        "  # X tranpose Y_n\n",
        "  XtY_n=cvxopt.matrix.trans(X)*Y_n\n",
        "\n",
        "  # Y_n tranpose X\n",
        "  YtX_n=cvxopt.matrix.trans(Y_n)*X\n",
        "\n",
        "  # Y_n transpose Z\n",
        "  YtZ_n=cvxopt.matrix.trans(Y_n)*Z\n",
        "\n",
        "  # Y_n tranpose Y_n\n",
        "  YtY_n=cvxopt.matrix.trans(Y_n)*Y_n\n",
        "  \n",
        "  # Obtain the voxels estimates independently\n",
        "  t1 = time.time()\n",
        "  theta_Est=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY ,P,tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "  theta_Est_n=minimize(PLS, theta0, args=(ZtX, ZtY_n, XtX, ZtZ, XtY_n, YtX_n, YtZ_n, XtZ, YtY_n ,P,tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "  t2 = time.time()\n",
        "  time_original = time_original + t2 - t1\n",
        "  \n",
        "  # Obtain the voxels estimates by reusing the estimate from one voxel as the initial estimate for the other\n",
        "  t1 = time.time()\n",
        "  theta_Est=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY ,P,tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "  theta_Est_n=minimize(PLS, theta_Est, args=(ZtX, ZtY_n, XtX, ZtZ, XtY_n, YtX_n, YtZ_n, XtZ, YtY_n ,P,tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "  t2 = time.time()\n",
        "  time_idea1 = time_idea1 + t2 - t1\n",
        "\n",
        "# Convert to averages\n",
        "time_original = time_original/n\n",
        "time_idea1 = time_idea1/n\n",
        "\n",
        "# Print\n",
        "print('Average Original estimation time:')\n",
        "print(time_original)\n",
        "print('Average Idea 1 estimation time:')\n",
        "print(time_idea1)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Original estimation time:\n",
            "0.10135724544525146\n",
            "Average Idea 1 estimation time:\n",
            "0.09602343797683716\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JP8KZC-Mamk",
        "colab_type": "text"
      },
      "source": [
        "### Idea 2: Penalize the difference between beta values in the objective function\n",
        "\n",
        "The PLS algorithm works by penalizing, at every voxel, the objective function:\n",
        "\n",
        "$$||Y-X\\beta+Z\\Lambda u+\\epsilon||^2_2 + ||u||^2_2$$\n",
        "\n",
        "Suppose, now that we wish to estimate the value at one voxel but already have beta values for one of it's neighbours. When the algorithm searches the parameter space, it stands to reason we want it to start by looking at beta values around the neighbouring voxels beta values. To do this we could add an additional term to the objective function:\n",
        "\n",
        "$$||Y-X\\beta+Z\\Lambda u+\\epsilon||^2_2 + ||u||^2_2 + ||\\beta - \\beta_n||^2_2$$\n",
        "\n",
        "Where $\\beta_n$ is the value of beta at the neighbouring voxel. This is the idea tested here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krn-dunmRTZl",
        "colab_type": "text"
      },
      "source": [
        "#### Return Beta from PLS\n",
        "\n",
        "This function returns the beta estimates for a given theta using PLS.\n",
        "\n",
        "---\n",
        "\n",
        "The following inputs are required for this function:\n",
        "\n",
        "---\n",
        "\n",
        " - **theta**: The parameter estimate.\n",
        " - **ZtX**: Z transpose multiplied by X.\n",
        " - **ZtY**: Z transpose multiplied by Y.\n",
        " - **XtX**: X transpose multiplied by X.\n",
        " - **ZtZ**: Z transpose multiplied by Z.\n",
        " - **XtY**: X transpose multiplied by Y.\n",
        " - **YtX**: Y transpose multiplied by X.\n",
        " - **YtZ**: Y transpose multiplied by Z.\n",
        " - **XtZ**: X transpose multiplied by Z.\n",
        " - **YtY**: Y transpose multiplied by Y.\n",
        " - **P**: The sparse permutation for Lamda'Z'ZLambda+I\n",
        " - **tinds**: A vector specifying how many times each theta parameter should be repeated. For example, if theta=[0.1,0.8,0.3] and theta_inds=[1,1,1,2,3,3], then the values to be mapped into the sparse matrix would be [0.1,0.1,0.1,0.8,0.3,0.3].\n",
        " - **r_inds**: The row indices of the elements mapped into the sparse matrix.\n",
        " - **c_inds**: The column indices of the elements mapped into the sparse matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEUPtjalRQpt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def PLS_getBeta(theta, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, P, tinds, rinds, cinds):\n",
        "\n",
        "    # Obtain Lambda\n",
        "    Lambda = mapping(theta, tinds, rinds, cinds)\n",
        "    \n",
        "    # Obtain Lambda'\n",
        "    Lambdat = spmatrix.trans(Lambda)\n",
        "\n",
        "    # Obtain Lambda'Z'Y and Lambda'Z'X\n",
        "    LambdatZtY = Lambdat*ZtY\n",
        "    LambdatZtX = Lambdat*ZtX\n",
        "    \n",
        "    # Set the factorisation to use LL' instead of LDL'\n",
        "    cholmod.options['supernodal']=2\n",
        "\n",
        "    # Obtain the cholesky decomposition\n",
        "    LambdatZtZLambda = Lambdat*ZtZ*Lambda\n",
        "    I = spmatrix(1.0, range(Lambda.size[0]), range(Lambda.size[0]))\n",
        "    chol_dict = sparse_chol(LambdatZtZLambda+I, perm=P, retF=True, retP=False, retL=False)\n",
        "    F = chol_dict['F']\n",
        "\n",
        "    # Obtain C_u (annoyingly solve writes over the second argument,\n",
        "    # whereas spsolve outputs)\n",
        "    Cu = LambdatZtY[P,:]\n",
        "    cholmod.solve(F,Cu,sys=4)\n",
        "\n",
        "    # Obtain RZX\n",
        "    RZX = LambdatZtX[P,:]\n",
        "    cholmod.solve(F,RZX,sys=4)\n",
        "\n",
        "    # Obtain RXtRX\n",
        "    RXtRX = XtX - matrix.trans(RZX)*RZX\n",
        "\n",
        "    # Obtain beta estimates (note: gesv also replaces the second\n",
        "    # argument)\n",
        "    betahat = XtY - matrix.trans(RZX)*Cu\n",
        "    lapack.posv(RXtRX, betahat)\n",
        "\n",
        "    return(betahat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-a7uYOZzPQ2W",
        "colab_type": "text"
      },
      "source": [
        "#### Neighbour-based PLS objective function\n",
        "\n",
        "The following function calculates the log likelihood based on the above objective function. It is the same as the original PLS algorithm in most respects, except it now also uses as input `beta_n`, the beta value from the run used on a neighbouring voxel. \n",
        "\n",
        "---\n",
        "\n",
        "The following inputs are required for this function:\n",
        "\n",
        "---\n",
        "\n",
        " - **theta**: The parameter estimate.\n",
        " - **betan**: The beta estimate for a neighbouring voxel.\n",
        " - **ZtX**: Z transpose multiplied by X.\n",
        " - **ZtY**: Z transpose multiplied by Y.\n",
        " - **XtX**: X transpose multiplied by X.\n",
        " - **ZtZ**: Z transpose multiplied by Z.\n",
        " - **XtY**: X transpose multiplied by Y.\n",
        " - **YtX**: Y transpose multiplied by X.\n",
        " - **YtZ**: Y transpose multiplied by Z.\n",
        " - **XtZ**: X transpose multiplied by Z.\n",
        " - **YtY**: Y transpose multiplied by Y.\n",
        " - **P**: The sparse permutation for Lamda'Z'ZLambda+I\n",
        " - **tinds**: A vector specifying how many times each theta parameter should be repeated. For example, if theta=[0.1,0.8,0.3] and theta_inds=[1,1,1,2,3,3], then the values to be mapped into the sparse matrix would be [0.1,0.1,0.1,0.8,0.3,0.3].\n",
        " - **r_inds**: The row indices of the elements mapped into the sparse matrix.\n",
        " - **c_inds**: The column indices of the elements mapped into the sparse matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E1CS6j7P1_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def PLSneighbour(theta, betan, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, P, tinds, rinds, cinds):\n",
        "\n",
        "    # Neighbour changes, the below is actually all that needs changing to account\n",
        "    # for the new objective function\n",
        "    XtX = XtX+ spmatrix(1.0, range(3), range(3))\n",
        "    XtY = XtY + betan\n",
        "    YtX = YtX + matrix.trans(betan)\n",
        "\n",
        "    # Return PLS on the newly adjusted data\n",
        "    return(PLS(theta, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY, P, tinds, rinds, cinds))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wW9A8JM3SRW7",
        "colab_type": "text"
      },
      "source": [
        "#### Correctness\n",
        "\n",
        "To check that we are still estimating the same quantity, here we estimate the neighbour first and then use the beta estimate from the neighbouring voxel to estimate the original toy datasets theta. As can be seen the resultant estimate is still relatively close.\n",
        "\n",
        "Interestingly, the only beta estimate that has changed is the intercept, which `R` had trouble estimating anyway (it took me a while to find a toy dataset were `R` was able to correctly estimate the intercept... it could have just been due to chance/my own p-hacking that it happened in this case). Perhaps this is due to the model I used for this example, in which there is a fixed effects intercept and then an intercept for both grouping factors... this could just be a difficult/impossible model intercept to estimate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCsbTsTpSRtl",
        "colab_type": "code",
        "outputId": "e72774bf-0167-461a-eaf7-9765a23a4d8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "source": [
        "# Compute theta estimate for neighbouring voxel\n",
        "theta_Est_n=minimize(PLS, theta0, args=(ZtX, ZtY_n, XtX, ZtZ, XtY_n, YtX_n, YtZ_n, XtZ, YtY_n ,P,tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "\n",
        "# Retrieve beta\n",
        "beta_Est_n = PLS_getBeta(theta_Est_n,ZtX, ZtY_n, XtX, ZtZ, XtY_n, YtX_n, YtZ_n, XtZ, YtY_n ,P,tinds, rinds, cinds)\n",
        "\n",
        "# Minimise for the original voxel using the neighbour function\n",
        "theta_Est=minimize(PLSneighbour, theta0, args=(beta_Est_n, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY ,P,tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "\n",
        "# Print results\n",
        "print(\"R estimated theta:\")\n",
        "print(theta_REst)\n",
        "print(\"Neighbourhood based estimated theta:\")\n",
        "print(theta_Est)\n",
        "\n",
        "# Print betas\n",
        "print(\"Beta (True)\")\n",
        "print(beta_True)\n",
        "print(\"Beta (R estimated)\")\n",
        "print(PLS_getBeta(theta_REst,ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY ,P,tinds, rinds, cinds))\n",
        "print(\"Beta (Neighbourhood estimated)\")\n",
        "print(PLS_getBeta(theta_Est,ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY ,P,tinds, rinds, cinds))\n",
        "\n",
        "# Double check we get the same as R\n",
        "#print(matrix(pd.read_csv('/Data/BLMM-testdata/estd_beta.csv',header=None).values))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "R estimated theta:\n",
            "[ 1.11365482  0.32356815  2.22872418  4.54822071 -0.17500295  0.42370817]\n",
            "Neighbourhood based estimated theta:\n",
            "[ 1.13052108  0.33596771  2.24880506  4.44852561 -0.15145034  0.43856462]\n",
            "Beta (True)\n",
            "[ 1]\n",
            "[ 2]\n",
            "[ 3]\n",
            "\n",
            "Beta (R estimated)\n",
            "[ 1.05e+00]\n",
            "[ 1.98e+00]\n",
            "[ 3.03e+00]\n",
            "\n",
            "Beta (Neighbourhood estimated)\n",
            "[ 8.45e-01]\n",
            "[ 1.98e+00]\n",
            "[ 3.03e+00]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aANxmrO3WIgP",
        "colab_type": "text"
      },
      "source": [
        "To further investigate whether this is a consistent result, I have run the code 100 times with different a neighbouring voxel in each run and recorded the average squared difference between the true value of beta and the estimated value of beta for both the neighbour approach and the original PLS algorithm.\n",
        "\n",
        "**Conclusion**: The previous supposed loss of efficiency was indeed a fluke - after running 100 different neighbouring voxel iterations it has become extremely clear that the added penalty significantly improves accuracy in the toy example. Main points of interest:\n",
        "\n",
        " - The standard PLS algorithm works well for a majority of cases but rarely beats the neighbourhood version. \n",
        " - In some cases the PLS algorithm can massively go wrong estimating the intercept, whereas the neighbourhood method appears more robust to this.\n",
        " - Of course, this is heavily dependent on the simulation and would need more investigation for standard neuroimaging data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3iC0KRtWIqq",
        "colab_type": "code",
        "outputId": "f75e4af3-2fd1-489b-d07e-973265014b9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# Average time\n",
        "diff_original = 0;\n",
        "diff_neighbour = 0;\n",
        "\n",
        "# Number of iterations\n",
        "n = 100;\n",
        "\n",
        "for i in range(n):\n",
        "  \n",
        "  # New neigbouring voxel response\n",
        "  beta_True_n = beta_n(beta_True)\n",
        "  Y_n = X*beta_True_n+Z*b_n(b_True)+cvxopt.normal(1000,1)\n",
        "  \n",
        "  #print(beta_True_n)\n",
        "  \n",
        "  # Z tranpose Y_n\n",
        "  ZtY_n=cvxopt.spmatrix.trans(Z)*Y_n\n",
        "\n",
        "  # X tranpose Y_n\n",
        "  XtY_n=cvxopt.matrix.trans(X)*Y_n\n",
        "\n",
        "  # Y_n tranpose X\n",
        "  YtX_n=cvxopt.matrix.trans(Y_n)*X\n",
        "\n",
        "  # Y_n transpose Z\n",
        "  YtZ_n=cvxopt.matrix.trans(Y_n)*Z\n",
        "\n",
        "  # Y_n tranpose Y_n\n",
        "  YtY_n=cvxopt.matrix.trans(Y_n)*Y_n\n",
        "  \n",
        "  # Obtain the voxels estimates independently\n",
        "  theta_Est=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY ,P,tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "  theta_Est_n=minimize(PLS, theta0, args=(ZtX, ZtY_n, XtX, ZtZ, XtY_n, YtX_n, YtZ_n, XtZ, YtY_n ,P,tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "  \n",
        "  # Get the beta value from seperate calculation\n",
        "  beta_Est_n = PLS_getBeta(theta_Est_n,ZtX, ZtY_n, XtX, ZtZ, XtY_n, YtX_n, YtZ_n, XtZ, YtY_n, P,tinds, rinds, cinds)\n",
        "  \n",
        "  #print(beta_Est_n)\n",
        "  \n",
        "  # Get average difference between estimated and truth\n",
        "  diff_original = np.linalg.norm(np.array(beta_Est_n-beta_True_n))**2 + diff_original\n",
        "  \n",
        "  #print(np.linalg.norm(np.array(beta_Est_n-beta_True_n))**2)\n",
        "  \n",
        "  # Obtain the voxels estimates by using the neighbouring approach\n",
        "  theta_Est= minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY ,P,tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "  beta_Est = PLS_getBeta(theta_Est,ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY ,P,tinds, rinds, cinds)\n",
        "  theta_Est_n = minimize(PLSneighbour, theta0, args=(beta_Est, ZtX, ZtY_n, XtX, ZtZ, XtY_n, YtX_n, YtZ_n, XtZ, YtY_n,P,tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "\n",
        "  # Get the beta value from seperate calculation\n",
        "  beta_Est_n = PLS_getBeta(theta_Est_n,ZtX, ZtY_n, XtX, ZtZ, XtY_n, YtX_n, YtZ_n, XtZ, YtY_n, P,tinds, rinds, cinds)\n",
        "  \n",
        "  #print(beta_Est_n)\n",
        "  #print(np.linalg.norm(np.array(beta_Est_n-beta_True_n))**2)\n",
        "  \n",
        "  # Get average difference between estimated and truth\n",
        "  diff_neighbour = np.linalg.norm(np.array(beta_Est_n-beta_True_n))**2 + diff_neighbour\n",
        "\n",
        "  #print('')\n",
        "# Convert to averages\n",
        "diff_original = diff_original/n\n",
        "diff_neighbour = diff_neighbour/n\n",
        "\n",
        "# Print\n",
        "print('Average Original squared difference from truth:')\n",
        "print(diff_original)\n",
        "print('Average Idea 2 squared difference from truth:')\n",
        "print(diff_neighbour)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Original squared difference from truth:\n",
            "26.414421899255796\n",
            "Average Idea 2 squared difference from truth:\n",
            "2.336129652312275\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAdjGFGxR_3w",
        "colab_type": "text"
      },
      "source": [
        "#### Time efficiency\n",
        "\n",
        "What is important here is the time taken to do this for both voxels. In the below, the two voxels are estimated 100 times twice, once reusing the estmate from the first voxel in estimating the second, and once computing the voxels completely seperately.\n",
        "\n",
        "**Conclusion:** Some significant time improvement was observed when using `Powell` (around 0.07s quicker) but not for `L-BFGS-B`. A more extensive test suite is needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8FROJLxR__g",
        "colab_type": "code",
        "outputId": "f22fad3c-929b-40c7-83e9-ac0a27504e14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# Average time\n",
        "time_original = 0;\n",
        "time_idea2 = 0;\n",
        "\n",
        "# Number of iterations\n",
        "n = 100;\n",
        "\n",
        "for i in range(n):\n",
        "  \n",
        "  # New neigbouring voxel response\n",
        "  Y_n = X*beta_n(beta_True)+Z*b_n(b_True)+cvxopt.normal(1000,1)\n",
        "  \n",
        "  # Z tranpose Y_n\n",
        "  ZtY_n=cvxopt.spmatrix.trans(Z)*Y_n\n",
        "\n",
        "  # X tranpose Y_n\n",
        "  XtY_n=cvxopt.matrix.trans(X)*Y_n\n",
        "\n",
        "  # Y_n tranpose X\n",
        "  YtX_n=cvxopt.matrix.trans(Y_n)*X\n",
        "\n",
        "  # Y_n transpose Z\n",
        "  YtZ_n=cvxopt.matrix.trans(Y_n)*Z\n",
        "\n",
        "  # Y_n tranpose Y_n\n",
        "  YtY_n=cvxopt.matrix.trans(Y_n)*Y_n\n",
        "  \n",
        "  # Obtain the voxels estimates independently\n",
        "  t1 = time.time()\n",
        "  theta_Est=minimize(PLS, theta0, args=(ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY ,P,tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "  theta_Est_n=minimize(PLS, theta0, args=(ZtX, ZtY_n, XtX, ZtZ, XtY_n, YtX_n, YtZ_n, XtZ, YtY_n ,P,tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "  t2 = time.time()\n",
        "  time_original = time_original + t2 - t1\n",
        "  \n",
        "  # Obtain the voxels estimates by using the neighbouring approach\n",
        "  t1 = time.time()\n",
        "  theta_Est_n=minimize(PLS, theta0, args=(ZtX, ZtY_n, XtX, ZtZ, XtY_n, YtX_n, YtZ_n, XtZ, YtY_n ,P,tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "  beta_Est_n = PLS_getBeta(theta_Est_n,ZtX, ZtY_n, XtX, ZtZ, XtY_n, YtX_n, YtZ_n, XtZ, YtY_n ,P,tinds, rinds, cinds)\n",
        "  theta_Est=minimize(PLSneighbour, theta0, args=(beta_Est_n, ZtX, ZtY, XtX, ZtZ, XtY, YtX, YtZ, XtZ, YtY ,P,tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x\n",
        "  t2 = time.time()\n",
        "  time_idea2 = time_idea2 + t2 - t1\n",
        "\n",
        "# Convert to averages\n",
        "time_original = time_original/n\n",
        "time_idea2 = time_idea2/n\n",
        "\n",
        "# Print\n",
        "print('Average Original estimation time:')\n",
        "print(time_original)\n",
        "print('Average Idea 2 estimation time:')\n",
        "print(time_idea2)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Original estimation time:\n",
            "0.09889775276184082\n",
            "Average Idea 2 estimation time:\n",
            "0.11362301111221314\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBr0APoAXKl3",
        "colab_type": "text"
      },
      "source": [
        "## Scaling up the computation (Random field)\n",
        "\n",
        "This section has several implemented ideas for scaling up the computation to compute several similar models at once. For simplicity it is assumed here that X and Z are the same across voxels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82u9AasMXRLz",
        "colab_type": "text"
      },
      "source": [
        "### Toy dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LTG_EYiZpjM",
        "colab_type": "text"
      },
      "source": [
        "#### Matrix Dimensions\n",
        "\n",
        "Below are the matrix dimensions used for this example. If the model has form:\n",
        "\n",
        "$$Y=X\\beta+Zb+\\epsilon$$ With $\\epsilon \\sim N(0,\\sigma^2I_n)$ and $b \\sim N(0,\\sigma^2D)$, then the dimensions of each matrix are as follows:\n",
        "\n",
        " - $Y$: $(n \\times 1)$\n",
        " - $X$: $(n \\times p)$\n",
        " - $\\beta$: $(p \\times 1)$\n",
        " - $Z$: $(n \\times q)$\n",
        " - $b$: $(q \\times 1)$\n",
        " - $\\epsilon$: $(n \\times 1)$\n",
        " - $D$: $(p\\times p)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LAfqqiCXO4k",
        "colab_type": "code",
        "outputId": "a0754c7c-44d0-427a-fa58-9cecbbdf0523",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "# Number of factors, random integer between 1 and 3\n",
        "r = np.random.randint(1,4)\n",
        "print(\"Number of grouping factors for random effects:\")\n",
        "print(r)\n",
        "\n",
        "# Number of levels, random number between 1 and 30\n",
        "nlevels = np.random.randint(1,31,size=(r))\n",
        "print(\"Number of levels for each factor:\")\n",
        "print(nlevels)\n",
        "\n",
        "# Number of parameters, random number between 1 and 5\n",
        "nparams = np.random.randint(1,6,size=(r))\n",
        "print(\"Number of parameters for each factor:\")\n",
        "print(nparams)\n",
        "\n",
        "# Dimension of D\n",
        "print(\"Dimension of D, q:\")\n",
        "q = np.sum(nlevels*nparams)\n",
        "print(q)\n",
        "\n",
        "# Number of fixed effects, random number between 1 and 30\n",
        "p = np.random.randint(1,31)\n",
        "print(\"Number of fixed effects:\")\n",
        "print(p)\n",
        "\n",
        "# Number of subjects, n\n",
        "n = 20*np.sum(nlevels)\n",
        "print(\"Number of subjects:\")\n",
        "print(n)\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of grouping factors for random effects:\n",
            "1\n",
            "Number of levels for each factor:\n",
            "[30]\n",
            "Number of parameters for each factor:\n",
            "[1]\n",
            "Dimension of D, q:\n",
            "30\n",
            "Number of fixed effects:\n",
            "22\n",
            "Number of subjects:\n",
            "600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57CJIErlbDS5",
        "colab_type": "text"
      },
      "source": [
        "#### Fixed Effects matrix (X)\n",
        "\n",
        "For simplicity, in this example $X$ is the same for all voxels. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wnJHsOxdUne",
        "colab_type": "code",
        "outputId": "55ae4319-6359-4e3f-9667-048e9bd09bb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "# Initialize empty x\n",
        "X = np.zeros((n,p))\n",
        "\n",
        "# First column is intercept\n",
        "X[:,0] = 1\n",
        "\n",
        "# Rest of the columns we will make random noise \n",
        "X[:,1:] = np.random.randn(n*(p-1)).reshape((n,(p-1)))\n",
        "\n",
        "# Image of the last 20 rows of X\n",
        "imshow(X[-20:-1,:])\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fa0ea907518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAASsAAAD8CAYAAAAv6IKXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIhIFhZWIREFhJSJRUFiJSBQUViIShf8DpSO+OReW7s8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaBzkwl0ekWr",
        "colab_type": "text"
      },
      "source": [
        "#### Random Effects matrix (Z)\n",
        "\n",
        "For simplicity, in this example $Z$ is the same for all voxels. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb1S4WdhepI9",
        "colab_type": "code",
        "outputId": "0e5c9d44-04c8-436f-8472-90af5583fdb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        }
      },
      "source": [
        "# We need to create a block of Z for each level of each factor\n",
        "for i in np.arange(r):\n",
        "  \n",
        "  for j in np.arange(nlevels[i]):\n",
        "    \n",
        "    # Initialize block\n",
        "    Zij = np.zeros((20,nparams[i]))\n",
        "    \n",
        "    # We give the first grouping factor an intercept\n",
        "    # and make the rest noise\n",
        "    if i==0:\n",
        "      \n",
        "      Zij[:,0] = 1\n",
        "      Zij[:,1:] = np.random.randn(20*(nparams[i]-1)).reshape((20,(nparams[i]-1)))\n",
        "      \n",
        "    else:\n",
        "      \n",
        "      Zij[:,:] = np.random.randn(20*nparams[i]).reshape((20,nparams[i]))\n",
        "    \n",
        "    # Convert to sparse\n",
        "    Zij = scipy.sparse.csr_matrix(Zij)\n",
        "    \n",
        "    # Initialize Z on first iteration and add to it after\n",
        "    if (i==0) & (j==0):\n",
        "      \n",
        "      Z = Zij\n",
        "\n",
        "    else:\n",
        "\n",
        "      # Add block to Z\n",
        "      Z = scipy.sparse.block_diag((Z, scipy.sparse.csr_matrix(Zij)))\n",
        "\n",
        "Z2 = sparse.COO.from_scipy_sparse(Z)\n",
        "\n",
        "# Create an image of Z'\n",
        "imshow(Z.toarray(), \\\n",
        "       interpolation='nearest', vmin=-5, vmax=5, aspect='auto')\n",
        "\n",
        "print(nlevels)\n",
        "print(nparams)\n",
        "print(Z.shape)\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[30]\n",
            "[1]\n",
            "(600, 30)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nr6yNSCcixgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTqsJgFrYP85",
        "colab_type": "text"
      },
      "source": [
        "#### Smooth random beta\n",
        "Smooth random beta image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUeftuE-YQjP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "8c17c5ae-8db3-44b8-8e69-4f58ecadefba"
      },
      "source": [
        "# Random 4D matrix (unsmoothed)\n",
        "beta_us = np.random.randn(1000*p).reshape(10,10,10,p)*20\n",
        "beta_us[3:5,3:5,3:5,3] = beta_us[3:5,3:5,3:5,3] + 100\n",
        "\n",
        "# Some random affine, not important for this simulation\n",
        "affine = np.diag([1, 1, 1, 1])\n",
        "beta_us_nii = nib.Nifti1Image(beta_us, affine)\n",
        "\n",
        "# Smoothed beta nifti\n",
        "beta_s_nii = nilearn.image.smooth_img(beta_us_nii, 5)\n",
        "\n",
        "# Final beta\n",
        "beta = beta_s_nii.get_fdata()\n",
        "\n",
        "# Show unsmoothed\n",
        "imshow(beta_us[3,:,:,3].reshape(10,10), \\\n",
        "                    interpolation='nearest', aspect='auto')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fa0ea0b9a20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOs4ge-_zgIr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "c902af04-b7f7-4272-8182-ca032efc9900"
      },
      "source": [
        "# Show smoothed\n",
        "imshow(beta[3,:,:,3].reshape(10,10), \\\n",
        "                    interpolation='nearest', aspect='auto')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fa0ea0f7160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSSrGcEtSMYZbkoox3JJUjOGWpGL+H9Rxu6fNLtu2AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vabjCMt0eMq",
        "colab_type": "text"
      },
      "source": [
        "#### Smooth random b\n",
        "Smooth random b image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bSmo7Af0ecp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "fb06cbed-fc4e-4a4a-afa1-1acab2d69f5b"
      },
      "source": [
        "# Random 4D matrix (unsmoothed)\n",
        "b_us = np.random.randn(1000*q).reshape(10,10,10,q)*20\n",
        "\n",
        "# Some random affine, not important for this simulation\n",
        "affine = np.diag([1, 1, 1, 1])\n",
        "b_us_nii = nib.Nifti1Image(b_us, affine)\n",
        "\n",
        "# Smoothed beta nifti\n",
        "b_s_nii = nilearn.image.smooth_img(b_us_nii, 5)\n",
        "\n",
        "# Final beta\n",
        "b = b_s_nii.get_fdata()\n",
        "\n",
        "# Show unsmoothed\n",
        "imshow(b_us[3,:,:,1].reshape(10,10), \\\n",
        "                    interpolation='nearest', aspect='auto')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fa0ea13ef60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8UW+JhBfCckAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXEPETmY0w4z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "85625448-86ea-4386-c92e-7949b34aebe1"
      },
      "source": [
        "# Show smoothed\n",
        "imshow(b[3,:,:,1].reshape(10,10), \\\n",
        "                    interpolation='nearest', aspect='auto')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fa0ea03f160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYgg3ABRDuAGgGMINAMX8L3OapqottQMyAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzdoPEEm6eUC",
        "colab_type": "text"
      },
      "source": [
        " #### Y vector (New response)\n",
        " \n",
        "Generate response for the whole field."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQokPoYh6eoY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reshape X\n",
        "X = X.reshape(1, X.shape[0], X.shape[1])\n",
        "\n",
        "# Reshape beta\n",
        "beta = beta.reshape(beta.shape[0]*beta.shape[1]*beta.shape[2],beta.shape[3],1)\n",
        "\n",
        "# Reshape Z (note: This step is slow because of the sparse to dense conversion;\n",
        "# it could probably be made quicker but this is only for one simulation at current)\n",
        "Ztmp = Z.toarray().reshape(1, Z.shape[0], Z.shape[1])\n",
        "\n",
        "# Reshape b\n",
        "b = b.reshape(b.shape[0]*b.shape[1]*b.shape[2],b.shape[3],1)\n",
        "\n",
        "# Generate Y\n",
        "Y = np.matmul(X,beta)+np.matmul(Ztmp,b) + np.random.randn(n,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Laa68OGOvyw",
        "colab_type": "text"
      },
      "source": [
        "Check Y looks reasonable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lTEeBqSOz1B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "18848f68-9f1e-4032-85c5-be8ef3aebbf9"
      },
      "source": [
        "print(Y.shape)\n",
        "\n",
        "Y_imageformat = Y.reshape((10,10,10,n))\n",
        "\n",
        "imshow(Y_imageformat[3,:,:,1].reshape(10,10), \\\n",
        "                    interpolation='nearest', aspect='auto')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 600, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fa0e9fa97f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-MhOA6XP-op",
        "colab_type": "text"
      },
      "source": [
        "### Transpose products\n",
        "\n",
        "Calculate X'Y, X'Z, X'Y, Y'Y, Y'Z, Y'X Z'Z, Z'X and Z'Y."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCFYLn3sQVU2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "09f1c9e0-fb54-4014-c6dd-46635a787c7c"
      },
      "source": [
        "# X'Z\\Z'X\n",
        "XtZ = np.matmul(X.transpose(0,2,1),Ztmp)\n",
        "ZtX = XtZ.transpose(0,2,1)\n",
        "\n",
        "# Z'Y\\Y'Z\n",
        "YtZ = np.matmul(Y.transpose(0,2,1),Ztmp)\n",
        "ZtY = YtZ.transpose(0,2,1)\n",
        "\n",
        "# Y'X/X'Y\n",
        "YtX = np.matmul(Y.transpose(0,2,1),X)\n",
        "XtY = YtX.transpose(0,2,1)\n",
        "\n",
        "# YtY\n",
        "YtY = np.matmul(Y.transpose(0,2,1),Y)\n",
        "\n",
        "# ZtZ\n",
        "ZtZ = np.matmul(Ztmp.transpose(0,2,1),Ztmp)\n",
        "\n",
        "# X'X\n",
        "XtX = np.matmul(X.transpose(0,2,1),X)\n",
        "\n",
        "\n",
        "print(XtZ.shape)\n",
        "print(ZtX.shape)\n",
        "\n",
        "print(XtY.shape)\n",
        "print(YtX.shape)\n",
        "\n",
        "print(YtZ.shape)\n",
        "print(ZtY.shape)\n",
        "\n",
        "print(XtX.shape)\n",
        "\n",
        "print(YtY.shape)\n",
        "\n",
        "print(ZtZ.shape)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 22, 30)\n",
            "(1, 30, 22)\n",
            "(1000, 22, 1)\n",
            "(1000, 1, 22)\n",
            "(1000, 1, 30)\n",
            "(1000, 30, 1)\n",
            "(1, 22, 22)\n",
            "(1000, 1, 1)\n",
            "(1, 30, 30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYxYXW3AUu3H",
        "colab_type": "text"
      },
      "source": [
        "#### Demonstration: Time taken just looping\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgah7SUnUt5k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e4786aca-4848-4e0b-a381-ae1d4ed6bb98"
      },
      "source": [
        "beta_est = np.zeros(beta.shape)\n",
        "\n",
        "for i in np.arange(beta.shape[0]):\n",
        "  \n",
        "  XtYtmp = XtY[i,:,:] \n",
        "  #minimize(PLS, theta0, args=(ZtX, ZtY_n, XtX, ZtZ, XtY_n, YtX_n, YtZ_n, XtZ, YtY_n ,P,tinds, rinds, cinds), method='L-BFGS-B', tol=1e-6).x"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n",
            "(22, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KowcFHBSSAIa",
        "colab_type": "text"
      },
      "source": [
        "#### Idea 3: Broadcast everything we **can**\n",
        "\n",
        "The title for this idea speaks for itself. The only operations that cannot be broadcast are those that must be done in `cvxopt`, the only for which it is absolutely must use `cvxopt` is the sparse cholesky decomposition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkCJOA2DkdNn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR2epLIRi95N",
        "colab_type": "text"
      },
      "source": [
        "#### Idea 4: Large sparse diagonals\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZNddOmJi-OX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}